{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SydneyCodeChallenge.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "26fecc8f5a1f4ceba2bff252b6bca03b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_df15ef1140a24cb393b9a7cb695fe3cd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_88cb421e32cf454aba6cedbbac3fe3c8",
              "IPY_MODEL_9de0ed61bf684d208f7fbf0f510fdc16"
            ]
          }
        },
        "df15ef1140a24cb393b9a7cb695fe3cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88cb421e32cf454aba6cedbbac3fe3c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_18b02ac94c714829af3f18bb711eb5d9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 838,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 838,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_df8abdcc3a78454d9455ff21cc443cde"
          }
        },
        "9de0ed61bf684d208f7fbf0f510fdc16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c04c27dbda4941af961ce96d137f7f98",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%|██████████| 838/838 [00:00&lt;00:00, 2177.88it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15abb33144f842b7959f6cac3551f187"
          }
        },
        "18b02ac94c714829af3f18bb711eb5d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "df8abdcc3a78454d9455ff21cc443cde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c04c27dbda4941af961ce96d137f7f98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15abb33144f842b7959f6cac3551f187": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a92648886b6d4a5eaece9f2167508803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_65bcc517cb824b6a8e5b204b954e9125",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_43a4544adba441f28e41b1de24beca88",
              "IPY_MODEL_335ad41501284857bdbd67360f8b7f8a"
            ]
          }
        },
        "65bcc517cb824b6a8e5b204b954e9125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "43a4544adba441f28e41b1de24beca88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dde839fdaf694c57aeeaeaf4f195894d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 105,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 105,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2d505469df08431b8be7af90c29cca39"
          }
        },
        "335ad41501284857bdbd67360f8b7f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4241b7c377c145ae8ac7b22b7edd33a9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%|██████████| 105/105 [00:04&lt;00:00, 25.30it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_885fae75606a468a9eae6e40950c679a"
          }
        },
        "dde839fdaf694c57aeeaeaf4f195894d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2d505469df08431b8be7af90c29cca39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4241b7c377c145ae8ac7b22b7edd33a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "885fae75606a468a9eae6e40950c679a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "823799ee1da14ff78d57fae4d063a294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_717d60b5c3e7498698199ce8f3281f05",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6944e7b69d6845dfb94582cb046959ec",
              "IPY_MODEL_06734d78518b4a0abd1abd7d63dedd66"
            ]
          }
        },
        "717d60b5c3e7498698199ce8f3281f05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6944e7b69d6845dfb94582cb046959ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a5e41150772e42dbad8333e119d9c9c0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 906,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 906,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2859040581804d949b4cb26cccabd724"
          }
        },
        "06734d78518b4a0abd1abd7d63dedd66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8034d5b05f5a4036b6aad45b62e9b5fb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%|██████████| 906/906 [03:17&lt;00:00,  4.58it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_35d0160b9df74f40a18299e27300db2f"
          }
        },
        "a5e41150772e42dbad8333e119d9c9c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2859040581804d949b4cb26cccabd724": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8034d5b05f5a4036b6aad45b62e9b5fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "35d0160b9df74f40a18299e27300db2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8713dcac9b0d4faf8b54c4a6ffb112a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_62b040d4c35048db8d902fdfc95776b4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a3e1f92785ad431080b49e3a51acd34a",
              "IPY_MODEL_50db2565dd044359b93a29fe9b086542"
            ]
          }
        },
        "62b040d4c35048db8d902fdfc95776b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a3e1f92785ad431080b49e3a51acd34a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_252ff694f8c74bfebbab11b2afb10382",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 114,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 114,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d8cfae6b1b374c999ea30e80ee990f87"
          }
        },
        "50db2565dd044359b93a29fe9b086542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9077d1b5e8eb48b88a3d90786edd7286",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%|██████████| 114/114 [03:17&lt;00:00,  1.73s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a19be472809440cda7fcfe45d7df2030"
          }
        },
        "252ff694f8c74bfebbab11b2afb10382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d8cfae6b1b374c999ea30e80ee990f87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9077d1b5e8eb48b88a3d90786edd7286": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a19be472809440cda7fcfe45d7df2030": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qC-BBgF30sa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import some libraries\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "#Import some sci-kit learn libraries to make a simple classifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-f1t9Ko0Xx3",
        "colab_type": "text"
      },
      "source": [
        "# Data merge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2sAsV9s4FEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Read in the labelled data\n",
        "dftrain=pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/aa/data_training.csv\",header=0)\n",
        "#And the target tweet ids for the solution (and the dummy classifcations)\n",
        "dfsolution=pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/aa/data_example_random.csv\",header=0)\n",
        "#Hydrate following the steps in the Data page.\n",
        "#Read in the hydrated data\n",
        "dfhydrate=pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/aa/train_data.csv\",header=0)\n",
        "#Read in the hydrated data for the target tweet ids\n",
        "dftarget=pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/aa/test_data.csv\",header=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruKwsHWz4GgR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "outputId": "635f1af0-afb2-47e5-ddf7-14d40fa732af"
      },
      "source": [
        "#Merge the two datasets (tweet ids from data_training and the tweets from hydrated_training). \n",
        "#Note: the number of tweetids availble to hydrate may have changed since the labelled dataset was made.\n",
        "df=pd.merge(left=dfhydrate,right=dftrain, left_on='id', right_on='tweetid')\n",
        "df_soltion=pd.merge(left=dftarget,right=dfsolution, left_on='id', right_on='tweetid',how='right')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coordinates</th>\n",
              "      <th>created_at</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>media</th>\n",
              "      <th>urls</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>id</th>\n",
              "      <th>in_reply_to_screen_name</th>\n",
              "      <th>in_reply_to_status_id</th>\n",
              "      <th>in_reply_to_user_id</th>\n",
              "      <th>lang</th>\n",
              "      <th>place</th>\n",
              "      <th>possibly_sensitive</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>reweet_id</th>\n",
              "      <th>retweet_screen_name</th>\n",
              "      <th>source</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_url</th>\n",
              "      <th>user_created_at</th>\n",
              "      <th>user_screen_name</th>\n",
              "      <th>user_default_profile_image</th>\n",
              "      <th>user_description</th>\n",
              "      <th>user_favourites_count</th>\n",
              "      <th>user_followers_count</th>\n",
              "      <th>user_friends_count</th>\n",
              "      <th>user_listed_count</th>\n",
              "      <th>user_location</th>\n",
              "      <th>user_name</th>\n",
              "      <th>user_screen_name.1</th>\n",
              "      <th>user_statuses_count</th>\n",
              "      <th>user_time_zone</th>\n",
              "      <th>user_urls</th>\n",
              "      <th>user_verified</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>tweet_label_int</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Wed Aug 02 05:51:00 +0000 2017</td>\n",
              "      <td>Research autism vaccines</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>892623748146249729</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
              "      <td>#Research into cause of #autism hasn't occurre...</td>\n",
              "      <td>https://twitter.com/RobinSmithAK/status/892623...</td>\n",
              "      <td>Fri Feb 06 19:09:22 +0000 2015</td>\n",
              "      <td>RobinSmithAK</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12007</td>\n",
              "      <td>339</td>\n",
              "      <td>535</td>\n",
              "      <td>71</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Robin Smith</td>\n",
              "      <td>RobinSmithAK</td>\n",
              "      <td>29849</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>892623748146249729</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Thu Feb 01 21:08:36 +0000 2018</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>959171650305404928</td>\n",
              "      <td>SocialPowerOne1</td>\n",
              "      <td>9.591479e+17</td>\n",
              "      <td>1.381089e+09</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/#!/download/ipad\" ...</td>\n",
              "      <td>@SocialPowerOne1 “Kenneth Copeland, member of ...</td>\n",
              "      <td>https://twitter.com/raygivens99/status/9591716...</td>\n",
              "      <td>Thu Jan 10 00:37:53 +0000 2013</td>\n",
              "      <td>raygivens99</td>\n",
              "      <td>False</td>\n",
              "      <td>I will decide for myself. I bookmark news &amp; op...</td>\n",
              "      <td>30024</td>\n",
              "      <td>263</td>\n",
              "      <td>1032</td>\n",
              "      <td>21</td>\n",
              "      <td>canada</td>\n",
              "      <td>Fair</td>\n",
              "      <td>raygivens99</td>\n",
              "      <td>53102</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>959171650305404928</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Fri Apr 06 05:12:12 +0000 2018</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>982123785124917248</td>\n",
              "      <td>ProfTimNoakes</td>\n",
              "      <td>9.819910e+17</td>\n",
              "      <td>5.493045e+08</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://www.tweetcaster.com\" rel=\"nofo...</td>\n",
              "      <td>@ProfTimNoakes stop saving heart attack deaths...</td>\n",
              "      <td>https://twitter.com/swm0904/status/98212378512...</td>\n",
              "      <td>Sat Apr 23 11:44:40 +0000 2011</td>\n",
              "      <td>swm0904</td>\n",
              "      <td>False</td>\n",
              "      <td>Don't believe everything you read. There is su...</td>\n",
              "      <td>304</td>\n",
              "      <td>88</td>\n",
              "      <td>329</td>\n",
              "      <td>7</td>\n",
              "      <td>Brisbane</td>\n",
              "      <td>Smoked</td>\n",
              "      <td>swm0904</td>\n",
              "      <td>9566</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>982123785124917248</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Tue May 15 03:03:06 +0000 2018</td>\n",
              "      <td>POTUS health medicine illness pharmacy drug dr...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://twitter.com/HealthRanger/status/996118...</td>\n",
              "      <td>0</td>\n",
              "      <td>996224425753133056</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
              "      <td>The first flu vaccine under #POTUS Trump was 9...</td>\n",
              "      <td>https://twitter.com/cureworks/status/996224425...</td>\n",
              "      <td>Sat May 24 20:00:55 +0000 2008</td>\n",
              "      <td>cureworks</td>\n",
              "      <td>False</td>\n",
              "      <td>Computer Underground Railroad Ent., Founder &amp; ...</td>\n",
              "      <td>30607</td>\n",
              "      <td>5289</td>\n",
              "      <td>6048</td>\n",
              "      <td>533</td>\n",
              "      <td>United States</td>\n",
              "      <td>J. Nayer Hardin</td>\n",
              "      <td>cureworks</td>\n",
              "      <td>83844</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://www.nayer.blogspot.com</td>\n",
              "      <td>False</td>\n",
              "      <td>996224425753133056</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Tue Apr 18 12:25:17 +0000 2017</td>\n",
              "      <td>DeliverForGood</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>854309857738051584</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
              "      <td>Vaccinating against HPV costs just $10-$25 per...</td>\n",
              "      <td>https://twitter.com/MalathyIyerTOI/status/8543...</td>\n",
              "      <td>Fri Oct 10 11:24:38 +0000 2014</td>\n",
              "      <td>MalathyIyerTOI</td>\n",
              "      <td>False</td>\n",
              "      <td>Senior Editor (Health) at The Times of India</td>\n",
              "      <td>4417</td>\n",
              "      <td>4618</td>\n",
              "      <td>3037</td>\n",
              "      <td>57</td>\n",
              "      <td>Mumbai</td>\n",
              "      <td>Malathy Iyer</td>\n",
              "      <td>MalathyIyerTOI</td>\n",
              "      <td>5264</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>854309857738051584</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8367</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Thu Aug 29 04:25:33 +0000 2019</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://twitter.com/A51FR3D/status/11669298589...</td>\n",
              "      <td>http://dlvr.it/RC4gNC</td>\n",
              "      <td>0</td>\n",
              "      <td>1166929858921779201</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"https://dlvrit.com/\" rel=\"nofollow\"&gt;d...</td>\n",
              "      <td>Pinterest to direct vaccine searches to health...</td>\n",
              "      <td>https://twitter.com/A51FR3D/status/11669298589...</td>\n",
              "      <td>Thu Aug 09 22:09:20 +0000 2007</td>\n",
              "      <td>A51FR3D</td>\n",
              "      <td>False</td>\n",
              "      <td>Managing Director @PMA_Accountants in Chingfor...</td>\n",
              "      <td>1497</td>\n",
              "      <td>534</td>\n",
              "      <td>1340</td>\n",
              "      <td>118</td>\n",
              "      <td>London, UK</td>\n",
              "      <td>Asif Patel</td>\n",
              "      <td>A51FR3D</td>\n",
              "      <td>75143</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://www.asifpatel.com</td>\n",
              "      <td>False</td>\n",
              "      <td>1166929858921779201</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8368</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Mon Oct 07 06:31:28 +0000 2019</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>1181094671763869696</td>\n",
              "      <td>CCbucko</td>\n",
              "      <td>1.181094e+18</td>\n",
              "      <td>8.660691e+17</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>@CCbucko The right doesn't know how use vaccin...</td>\n",
              "      <td>https://twitter.com/jlebrech/status/1181094671...</td>\n",
              "      <td>Thu Apr 09 13:46:57 +0000 2009</td>\n",
              "      <td>jlebrech</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12638</td>\n",
              "      <td>884</td>\n",
              "      <td>2360</td>\n",
              "      <td>22</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Joseph#</td>\n",
              "      <td>jlebrech</td>\n",
              "      <td>36400</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>1181094671763869696</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8369</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Mon Oct 07 08:54:25 +0000 2019</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://twitter.com/drg1985/status/11811306500...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19</td>\n",
              "      <td>1181130650033807361</td>\n",
              "      <td>drg1985</td>\n",
              "      <td>1.181129e+18</td>\n",
              "      <td>2.648519e+08</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>@Twitter @SimonHarrisTD ..apologies for the re...</td>\n",
              "      <td>https://twitter.com/drg1985/status/11811306500...</td>\n",
              "      <td>Sat Mar 12 15:21:34 +0000 2011</td>\n",
              "      <td>drg1985</td>\n",
              "      <td>False</td>\n",
              "      <td>Cancer researcher, physicist, scoundrel. Autho...</td>\n",
              "      <td>19680</td>\n",
              "      <td>19778</td>\n",
              "      <td>1886</td>\n",
              "      <td>238</td>\n",
              "      <td>Dublin / Oxford</td>\n",
              "      <td>Dr David Robert Grimes</td>\n",
              "      <td>drg1985</td>\n",
              "      <td>10257</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://www.davidrobertgrimes.com</td>\n",
              "      <td>True</td>\n",
              "      <td>1181130650033807361</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8370</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Wed Aug 28 17:52:36 +0000 2019</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://twitter.com/thehill/status/11667689183...</td>\n",
              "      <td>4</td>\n",
              "      <td>1166770571343843328</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>Why are people looking for vaccine info on Pin...</td>\n",
              "      <td>https://twitter.com/Walton_Emily/status/116677...</td>\n",
              "      <td>Thu Feb 04 07:07:01 +0000 2010</td>\n",
              "      <td>Walton_Emily</td>\n",
              "      <td>False</td>\n",
              "      <td>I tweet about higher ed, Idaho, politics, #gun...</td>\n",
              "      <td>17466</td>\n",
              "      <td>3888</td>\n",
              "      <td>2528</td>\n",
              "      <td>82</td>\n",
              "      <td>Boise, Idaho</td>\n",
              "      <td>Emily Walton 👩‍⚖️</td>\n",
              "      <td>Walton_Emily</td>\n",
              "      <td>28189</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>1166770571343843328</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8371</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Sun Oct 06 18:13:59 +0000 2019</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1180909080505917440</td>\n",
              "      <td>girlfreddy</td>\n",
              "      <td>1.180908e+18</td>\n",
              "      <td>2.365454e+07</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>@ChuckWendig Annnd there's a vaccine shortage ...</td>\n",
              "      <td>https://twitter.com/girlfreddy/status/11809090...</td>\n",
              "      <td>Tue Mar 10 20:52:21 +0000 2009</td>\n",
              "      <td>girlfreddy</td>\n",
              "      <td>False</td>\n",
              "      <td>Jill of all trades. She/her. Almost 60 and sti...</td>\n",
              "      <td>160591</td>\n",
              "      <td>643</td>\n",
              "      <td>725</td>\n",
              "      <td>2</td>\n",
              "      <td>Manitoba, Canada</td>\n",
              "      <td>Screaming Hairy Armadillo (Freddie for short)</td>\n",
              "      <td>girlfreddy</td>\n",
              "      <td>46290</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>1180909080505917440</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8372 rows × 36 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     coordinates  ... tweet_label_int\n",
              "0            NaN  ...               0\n",
              "1            NaN  ...               0\n",
              "2            NaN  ...               0\n",
              "3            NaN  ...               1\n",
              "4            NaN  ...               0\n",
              "...          ...  ...             ...\n",
              "8367         NaN  ...               0\n",
              "8368         NaN  ...               0\n",
              "8369         NaN  ...               0\n",
              "8370         NaN  ...               0\n",
              "8371         NaN  ...               0\n",
              "\n",
              "[8372 rows x 36 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMYzbrqYYQry",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei9jCEkWYSb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get all sentences\n",
        "keeps = [chr(i) for i in range(97,123)] #+ [\"0\",'1','2','3','4','5','6','7','8','9']\n",
        "all_sentences=[]\n",
        "for review in df.text + df_soltion.text:\n",
        "    if review is np.NaN:\n",
        "        continue\n",
        "    all_sentences.extend(list(review.lower()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxpoqyCybFtB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68b0a7b7-7aba-4f87-e319-00e2d4e9b98c"
      },
      "source": [
        "len(all_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "303959"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcfXABESYhrT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ad63f628-66cd-47e1-e58e-544fb0cf65e1"
      },
      "source": [
        "# generate romove word list\n",
        "remove_char = []\n",
        "all_char_list = list(set(all_sentences))\n",
        "for i in all_char_list:\n",
        "  if i not in keeps:\n",
        "    remove_char.append(i)\n",
        "\n",
        "remove_chars = \"\".join(remove_char)\n",
        "print(remove_chars)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2ô​👉😂🙄😷🌻=🚨[♥🇺/😢💖👶😅 1…😍👈🤣➕👊🇦‍😮ì👀🍵🇨🤜8⚡‘⚠â👌🙏_ ;😉]💕😁#🕉🦠—,~👇🇧😀💉7🏼🗓4🤓😡€!🏽🍀.ê™💀–$0⁩↦3🐱🇸📹(✅*😥⁦📊?👍📋¡♂🤪-🦋&💪📢6\r\"’|£🤛🏻💊❄🍪😇à🌐è👩💛💯▶⃣🐔»🐾💧😎👏🤔5👆é'🩹‼)❤😳⚕➖\n",
            "🤱👨😺✨🌎😤🚶ú😔🇳🌍⚛+✌á%í9🇬:🤷🔗🤦ó•😊@“”♀🤢️\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntT8TgG9f7ZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import re\n",
        "# def remove_urls (vTEXT):\n",
        "#     vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
        "#     return(vTEXT)\n",
        "\n",
        "# def remove_user(vTEXT):\n",
        "#     # vTEXT = re.sub(r'(@)(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
        "#     vTEXT = re.sub(r'(@)(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
        "#     return(vTEXT)\n",
        "# print( remove_user(\"this is a test https://sdfs.sdfsdf.com/sdfsdf/sdfsdf/sd/sdfsdfs?bob=%20tree&jef=man lets see this too https://sdfsdf.fdf.com/sdf/f end @sad213_sasd as\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zGytiGLY1Yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import stem\n",
        "import string\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "Porter = stem.PorterStemmer()\n",
        "import re\n",
        "def remove_urls (vTEXT):\n",
        "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
        "    return(vTEXT)\n",
        "\n",
        "def remove_user(vTEXT):\n",
        "    # vTEXT = re.sub(r'(@)(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
        "    vTEXT = re.sub(r'(@)(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
        "    return(vTEXT)\n",
        "\n",
        "#token/stem/stopword_remove/ porcess\n",
        "def data_pre(sentence_list):\n",
        "  sentences = []\n",
        "  # this is a-z and space\n",
        "  for sentence in sentence_list:\n",
        "    if sentence is np.NaN:\n",
        "          sentence=''\n",
        "    # create a remove table for invalid character\n",
        "    remove = remove_chars\n",
        "    # There are some useless char in the sentence, so here I add and delete the data\n",
        "    replace = \" \" * len(remove)\n",
        "    # make a translate table\n",
        "    remove_table = str.maketrans(remove , replace)\n",
        "    #remove url\n",
        "    senetence_removeURL=remove_urls(sentence)\n",
        "    #REMOVE USER\n",
        "    senetence_removeUrlAndUser=remove_user(senetence_removeURL)\n",
        "    # clean punctuation and digits\n",
        "    sentence_replace = senetence_removeUrlAndUser.lower().translate(remove_table)\n",
        "    #clean stopwords and tokenize\n",
        "    sentence_replace = remove_stopwords(sentence_replace).split()\n",
        "    #remove meaningless tokens  also remove some HTML language\n",
        "    sentence_new = []\n",
        "    for token in sentence_replace:\n",
        "      # I think if a word are longer than 20, it have a great possiblity as a invalid word, and shorter than 2,it may be a stopword\n",
        "      # so i only get the word that the length are form 2 to 20.\n",
        "      if len(token) > 2 and len(token)<20:\n",
        "        # lemmatizer, let the word become it's original shape, had --> have\n",
        "        sentence_new.append(Porter.stem(token))\n",
        "    sentences.append(sentence_new)\n",
        "  return sentences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf_MpjhFY3_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# used the data_pre method that defind above\n",
        "train_data = data_pre(df.text)\n",
        "test_data = data_pre(df_soltion.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQXfS_LVcDZT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2162e1c5-6ed6-4228-878e-73b22a538d6e"
      },
      "source": [
        "print(df_soltion.text[13])\n",
        "print(test_data[13])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Opinion | Pinterest takes the right step toward curbing misinformation on vaccines - The Washington Post https://t.co/8rp99QK3Jk\n",
            "['opinion', 'pinterest', 'take', 'right', 'step', 'curb', 'misinform', 'vaccin', 'washington', 'post']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhLV3_aXYqrA",
        "colab_type": "text"
      },
      "source": [
        "#  split dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QcGLgqNY2JE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train_data,df.tweet_label_int, test_size=0.1,\n",
        "                                                    random_state=42) # so we get the same results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J7rQ3BMJTiL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "c2b88047-2ef4-4788-e998-596aeb96ccc6"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['candid',\n",
              " 'senat',\n",
              " 'nsw',\n",
              " 'ensur',\n",
              " 'jim',\n",
              " 'molan',\n",
              " 'vote',\n",
              " 'paper',\n",
              " 'go',\n",
              " 'backward',\n",
              " 'anti',\n",
              " 'vaxxer',\n",
              " 'fraser',\n",
              " 'an',\n",
              " 'scrape']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu1WT-dTDTC_",
        "colab_type": "text"
      },
      "source": [
        "# Imbalence label refit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g_euaVVDWoE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b4b73f0a-bb2b-43ac-9ca4-00d185862a60"
      },
      "source": [
        "from imblearn.pipeline import make_pipeline\n",
        "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
        "def dummy(doc):\n",
        "    return doc\n",
        "make_tfidf=Pipeline([('vect', CountVectorizer(lowercase=False,tokenizer=dummy,\n",
        "                    preprocessor=dummy,binary=True,ngram_range=(1, 2))),\n",
        "                ('tfidf', TfidfTransformer(use_idf=True))])\n",
        "\n",
        "All_tfidf=make_tfidf.fit(train_data+test_data)\n",
        "training_tfidf=make_tfidf.transform(X_train)\n",
        "training_target=y_train\n",
        "\n",
        "\n",
        "smt = SMOTE(random_state=777, k_neighbors=1)\n",
        "X_SMOTE, y_SMOTE = smt.fit_sample(training_tfidf, training_target)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5bj7JZ9tTf8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b66d7d07-8aa1-48b4-a821-6f0b14c0cb21"
      },
      "source": [
        "X_SMOTE[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x86715 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 29 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zprRuXw7NHmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_tfidf=make_tfidf.transform(X_test)\n",
        "test_tfidf=make_tfidf.transform(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G73G2n2KKikF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e159890d-18b7-41ad-bcb2-b15f162b081a"
      },
      "source": [
        "import collections\n",
        "collections.Counter(y_SMOTE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 6663, 1: 6663})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wCfXCvTXBXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXawgRB_Tesv",
        "colab_type": "text"
      },
      "source": [
        "# TFIDF(SVM-NB-DT-RF-ADA-GDBT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhcCJhUV65mg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dummy(doc):\n",
        "    return doc\n",
        "svm = Pipeline([('vect', CountVectorizer(lowercase=False,tokenizer=dummy,\n",
        "                    preprocessor=dummy)),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', LinearSVC(class_weight='balanced'))\n",
        "               ])\n",
        "\n",
        "nb = Pipeline([('vect', CountVectorizer(lowercase=False,tokenizer=dummy,\n",
        "                    preprocessor=dummy)),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', MultinomialNB())\n",
        "               ])\n",
        "\n",
        "dt = Pipeline([('vect', CountVectorizer(lowercase=False,tokenizer=dummy,\n",
        "                    preprocessor=dummy)),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', DecisionTreeClassifier(class_weight='balanced'))\n",
        "               ])\n",
        "\n",
        "rf = Pipeline([('vect', CountVectorizer(lowercase=False,tokenizer=dummy,\n",
        "                    preprocessor=dummy)),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', RandomForestClassifier(class_weight='balanced'))\n",
        "               ])\n",
        "#from sklearn.ensemble import AdaBoostClassifier\n",
        "adab = Pipeline([('vect', CountVectorizer(lowercase=False,tokenizer=dummy,\n",
        "                    preprocessor=dummy)),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', AdaBoostClassifier())\n",
        "               ])\n",
        "\n",
        "gdbt = Pipeline([('vect', CountVectorizer(lowercase=False,tokenizer=dummy,\n",
        "                    preprocessor=dummy)),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', GradientBoostingClassifier())\n",
        "               ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ENCu8CT8v_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dummy(doc):\n",
        "    return doc\n",
        "vect=CountVectorizer(lowercase=False,\n",
        "                    tokenizer=dummy,\n",
        "                    preprocessor=dummy)\n",
        "x=vect.fit_transform(train_data)\n",
        "len(vect.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka-qmdNjUtgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Grid search parameters\n",
        "param_grid = [{'vect__binary': [True],\n",
        "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "               'tfidf__use_idf': [True, False],\n",
        "              },\n",
        "              {'vect__binary': [False],\n",
        "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "               'tfidf__use_idf': [True, False],\n",
        "              }\n",
        "             ]\n",
        "# Grid search parameters\n",
        "param_grid_svm = [{'vect__binary': [True],\n",
        "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "               'tfidf__use_idf': [True, False],\n",
        "               'clf__C':[10,1,0.1],\n",
        "               'clf__penalty':['l2']\n",
        "              },\n",
        "              {'vect__binary': [False],\n",
        "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "               'tfidf__use_idf': [True, False],\n",
        "               'clf__C':[10,1,0.1],\n",
        "               'clf__penalty':['l2']\n",
        "              }\n",
        "             ]\n",
        "\n",
        "\n",
        "param_grid_decision = [{'vect__binary': [True],\n",
        "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "               'tfidf__use_idf': [True, False],\n",
        "               'clf__max_depth':[2,4,8,16,32,None],\n",
        "              },\n",
        "              {'vect__binary': [False],\n",
        "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "               'tfidf__use_idf': [True, False],\n",
        "               'clf__max_depth':[2,4,8,16,32,None],\n",
        "              }\n",
        "             ]\n",
        "\n",
        "param_grid_rf = [{'vect__binary': [True],\n",
        "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "               'tfidf__use_idf': [True, False],\n",
        "               'clf__max_depth':[2,4,8,16,32,None],\n",
        "               'clf__n_estimators': [16,32,64,100],\n",
        "              },\n",
        "              {'vect__binary': [False],\n",
        "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "               'tfidf__use_idf': [True, False],\n",
        "               'clf__max_depth':[2,4,8,16,32,None],\n",
        "               'clf__n_estimators': [16,32,64,100],\n",
        "              }\n",
        "             ]\n",
        "\n",
        "param_grid_boosting = [{'vect__binary': [True],\n",
        "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "               'tfidf__use_idf': [True, False],\n",
        "               'clf__n_estimators': [64,100,150],\n",
        "              },\n",
        "              {'vect__binary': [False],\n",
        "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "               'tfidf__use_idf': [True, False],\n",
        "               'clf__n_estimators': [64,100,150],\n",
        "              }\n",
        "             ]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWgujjlGr_u4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find best parameters for MNB and SVM\n",
        "def GridSearchCV_customed(scoreMethod=None):\n",
        "    # gs_mnb_f1 = GridSearchCV(nb, param_grid,scoring=scoreMethod)\n",
        "    # gs_mnb_f1.fit(X_train, y_train)\n",
        "    # print('\\nMNB best params:\\n', gs_mnb_f1.best_params_)\n",
        "    # print('\\nMNB test result:\\n', classification_report(y_test, gs_mnb_f1.predict(X_test)))\n",
        "    # print('\\nMNB train result:\\n', classification_report(y_train, gs_mnb_f1.predict(X_train)))\n",
        "\n",
        "    # gs_svm_f1 = GridSearchCV(svm, param_grid_svm,scoring=scoreMethod)\n",
        "    # gs_svm_f1.fit(X_train, y_train)\n",
        "    # print('\\nSVM best params:\\n', gs_svm_f1.best_params_)\n",
        "    # print('\\nSVM test result:\\n', classification_report(y_test, gs_svm_f1.predict(X_test)))\n",
        "    # print('\\nSVM train result:\\n', classification_report(y_train, gs_svm_f1.predict(X_train)))\n",
        "\n",
        "    # dt_f1 = GridSearchCV(dt, param_grid_decision,scoring=scoreMethod)\n",
        "    # dt_f1.fit(X_train, y_train)\n",
        "    # print('\\nDT best params:\\n', dt_f1.best_params_)\n",
        "    # print('\\nDT test result:\\n', classification_report(y_test, dt_f1.predict(X_test)))\n",
        "    # print('\\nDT train result:\\n', classification_report(y_train, dt_f1.predict(X_train)))\n",
        "\n",
        "    rf_f1 = GridSearchCV(rf, param_grid_rf,scoring=scoreMethod)\n",
        "    rf_f1.fit(X_train, y_train)\n",
        "    print('\\nRF best params:\\n', rf_f1.best_params_)\n",
        "    print('\\nRF test result:\\n', classification_report(y_test, rf_f1.predict(X_test)))\n",
        "    print('\\nRF train result:\\n', classification_report(y_train, rf_f1.predict(X_train)))\n",
        "\n",
        "\n",
        "    # adab_f1 = GridSearchCV(adab, param_grid_boosting,scoring=scoreMethod)\n",
        "    # adab_f1.fit(X_train, y_train)\n",
        "    # print('\\nAdaBoost best params:\\n', adab_f1.best_params_)\n",
        "    # print('\\nAdaBoost test result:\\n', classification_report(y_test, adab_f1.predict(X_test)))\n",
        "    # print('\\nAdaBoost train result:\\n', classification_report(y_train, adab_f1.predict(X_train)))\n",
        "\n",
        "    # gdbt_f1 = GridSearchCV(gdbt, param_grid_boosting,scoring=scoreMethod)\n",
        "    # gdbt_f1.fit(X_train, y_train)\n",
        "    # print('\\nGDBoost best params:\\n', gdbt_f1.best_params_)\n",
        "    # print('\\nGDBoost test result:\\n', classification_report(y_test, gdbt_f1.predict(X_test)))\n",
        "    # print('\\nGDBoost train result:\\n', classification_report(y_train, gdbt_f1.predict(X_train)))\n",
        "    return rf_f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1vrJ9YFzPu2",
        "colab_type": "text"
      },
      "source": [
        "## f1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvVUIovRYkgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#F1\n",
        "GridSearchCV_customed()\n",
        "\n",
        "#SVM best params:\n",
        "#  {'clf__C': 10, 'clf__penalty': 'l2', 'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 2)}\n",
        "\n",
        "# DT best params:\n",
        "#  {'clf__max_depth': None, 'tfidf__use_idf': False, 'vect__binary': True, 'vect__ngram_range': (1, 1)}\n",
        "\n",
        "# RF best params:\n",
        "#  {'clf__max_depth': 32, 'clf__n_estimators': 64, 'tfidf__use_idf': True, 'vect__binary': False, 'vect__ngram_range': (1, 2)}\n",
        "\n",
        "# AdaBoost best params:\n",
        "#  {'clf__n_estimators': 64, 'tfidf__use_idf': True, 'vect__binary': False, 'vect__ngram_range': (1, 1)}\n",
        "\n",
        "# GDBoost best params:\n",
        "#  {'clf__n_estimators': 150, 'tfidf__use_idf': False, 'vect__binary': False, 'vect__ngram_range': (1, 2)}\n",
        "\n",
        "\n",
        "# SVM test result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.92      0.94      0.93       736\n",
        "#            1       0.46      0.38      0.42       102\n",
        "\n",
        "#     accuracy                           0.87       838\n",
        "#    macro avg       0.69      0.66      0.67       838\n",
        "# weighted avg       0.86      0.87      0.86       838\n",
        "\n",
        "# DT test result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.92      0.87      0.90       736\n",
        "#            1       0.33      0.44      0.38       102\n",
        "\n",
        "#     accuracy                           0.82       838\n",
        "#    macro avg       0.62      0.66      0.64       838\n",
        "# weighted avg       0.85      0.82      0.83       838\n",
        "\n",
        "\n",
        "# RF test result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.92      0.96      0.94       736\n",
        "#            1       0.56      0.36      0.44       102\n",
        "\n",
        "#     accuracy                           0.89       838\n",
        "#    macro avg       0.74      0.66      0.69       838\n",
        "# weighted avg       0.87      0.89      0.88       838\n",
        "\n",
        "\n",
        "\n",
        "# AdaBoost test result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.91      0.98      0.94       736\n",
        "#            1       0.69      0.30      0.42       102\n",
        "\n",
        "#     accuracy                           0.90       838\n",
        "#    macro avg       0.80      0.64      0.68       838\n",
        "# weighted avg       0.88      0.90      0.88       838\n",
        "\n",
        "\n",
        "# GDBoost test result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.90      0.99      0.94       736\n",
        "#            1       0.75      0.21      0.32       102\n",
        "\n",
        "#     accuracy                           0.89       838\n",
        "#    macro avg       0.82      0.60      0.63       838\n",
        "# weighted avg       0.88      0.89      0.87       838"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-vFUziizSY9",
        "colab_type": "text"
      },
      "source": [
        "## balanced_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGsn35o6aajg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "# scoring_ = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n",
        "\n",
        "GridSearchCV_customed(scoreMethod='balanced_accuracy')\n",
        "\n",
        "# MNB best params:\n",
        "#  {'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 2)}\n",
        "\n",
        "#  SVM best params:\n",
        "#  {'clf__C': 0.1, 'clf__penalty': 'l2', 'tfidf__use_idf': False, 'vect__binary': True, 'vect__ngram_range': (1, 2)}\n",
        "\n",
        "#  DT best params:\n",
        "#  {'clf__max_depth': None, 'tfidf__use_idf': True, 'vect__binary': False, 'vect__ngram_range': (1, 1)}\n",
        "\n",
        "#  RF best params:\n",
        "#  {'clf__max_depth': 8, 'clf__n_estimators': 100, 'tfidf__use_idf': False, 'vect__binary': False, 'vect__ngram_range': (1, 1)}\n",
        "\n",
        "# AdaBoost best params:\n",
        "#  {'clf__n_estimators': 150, 'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 2)}\n",
        "\n",
        "\n",
        "# GDBoost best params:\n",
        "#  {'clf__n_estimators': 150, 'tfidf__use_idf': True, 'vect__binary': False, 'vect__ngram_range': (1, 2)}\n",
        "\n",
        "# SVM test result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.95      0.85      0.90       736\n",
        "#            1       0.39      0.68      0.49       102\n",
        "\n",
        "#     accuracy                           0.83       838\n",
        "#    macro avg       0.67      0.76      0.70       838\n",
        "# weighted avg       0.88      0.83      0.85       838\n",
        "\n",
        "\n",
        "# RF test result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.94      0.89      0.91       736\n",
        "#            1       0.42      0.59      0.49       102\n",
        "\n",
        "#     accuracy                           0.85       838\n",
        "#    macro avg       0.68      0.74      0.70       838\n",
        "# weighted avg       0.88      0.85      0.86       838\n",
        "\n",
        "\n",
        "# AdaBoost test result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.91      0.95      0.93       736\n",
        "#            1       0.49      0.36      0.42       102\n",
        "\n",
        "#     accuracy                           0.88       838\n",
        "#    macro avg       0.70      0.65      0.67       838\n",
        "# weighted avg       0.86      0.88      0.87       838\n",
        "\n",
        "\n",
        "# GDBoost test result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.91      0.99      0.94       736\n",
        "#            1       0.72      0.25      0.38       102\n",
        "\n",
        "#     accuracy                           0.90       838\n",
        "#    macro avg       0.81      0.62      0.66       838\n",
        "# weighted avg       0.88      0.90      0.88       838"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iU6z4BXzVgf",
        "colab_type": "text"
      },
      "source": [
        "## jaccard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_VbHFAryO17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf=GridSearchCV_customed(scoreMethod='jaccard')\n",
        "\n",
        "\n",
        "\n",
        "# MNB best params:\n",
        "#  {'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 1)}\n",
        "\n",
        "#  SVM best params:\n",
        "#  {'clf__C': 0.1, 'clf__penalty': 'l2', 'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 2)}\n",
        "\n",
        "\n",
        "# DT best params:\n",
        "#  {'clf__max_depth': None, 'tfidf__use_idf': True, 'vect__binary': False, 'vect__ngram_range': (1, 1)}\n",
        "\n",
        "\n",
        "# RF best params:\n",
        "#  {'clf__max_depth': 16, 'clf__n_estimators': 100, 'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 2)}\n",
        "\n",
        "# AdaBoost best params:\n",
        "#  {'clf__n_estimators': 150, 'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 2)}\n",
        "\n",
        "# GDBoost best params:\n",
        "#  {'clf__n_estimators': 150, 'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 2)}\n",
        "\n",
        "# SVM test result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.94      0.90      0.92       736\n",
        "#            1       0.44      0.56      0.49       102\n",
        "\n",
        "#     accuracy                           0.86       838\n",
        "#    macro avg       0.69      0.73      0.70       838\n",
        "# weighted avg       0.88      0.86      0.87       838\n",
        "\n",
        "\n",
        "# RF test result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.93      0.92      0.93       736\n",
        "#            1       0.48      0.51      0.49       102\n",
        "\n",
        "#     accuracy                           0.87       838\n",
        "#    macro avg       0.70      0.72      0.71       838\n",
        "# weighted avg       0.88      0.87      0.87       838\n",
        "\n",
        "\n",
        "# AdaBoost test result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.91      0.95      0.93       736\n",
        "#            1       0.49      0.36      0.42       102\n",
        "\n",
        "#     accuracy                           0.88       838\n",
        "#    macro avg       0.70      0.65      0.67       838\n",
        "# weighted avg       0.86      0.88      0.87       838\n",
        "\n",
        "\n",
        "# GDBoost test result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.90      0.98      0.94       736\n",
        "#            1       0.67      0.24      0.35       102\n",
        "\n",
        "#     accuracy                           0.89       838\n",
        "#    macro avg       0.78      0.61      0.64       838\n",
        "# weighted avg       0.87      0.89      0.87       838"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlqcv01nhMis",
        "colab_type": "text"
      },
      "source": [
        "# Stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBtp3aCSicCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  {'clf__C': 0.1, 'clf__penalty': 'l2', 'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 2)}\n",
        "svm = Pipeline([\n",
        "                ('clf', LinearSVC(class_weight='balanced',C= 0.1, penalty= 'l2'))\n",
        "               ])\n",
        "\n",
        "#  {'clf__max_depth': 16, 'clf__n_estimators': 100, 'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 2)}\n",
        "rf = Pipeline([\n",
        "                ('clf', RandomForestClassifier(class_weight='balanced',max_depth= 16,n_estimators=100))\n",
        "               ])\n",
        "\n",
        "#  {'clf__n_estimators': 150, 'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 2)}\n",
        "#from sklearn.ensemble import AdaBoostClassifier\n",
        "adab = Pipeline([\n",
        "                ('clf', AdaBoostClassifier(n_estimators= 150))\n",
        "               ])\n",
        "#  {'clf__n_estimators': 150, 'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 2)}\n",
        "gdbt = Pipeline([\n",
        "                ('clf', GradientBoostingClassifier(n_estimators= 150))\n",
        "               ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eub78EQub_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm.fit(X_SMOTE, y_SMOTE)\n",
        "svm.predict_proba(val_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rpebd48ucQO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b07e1223-8fca-4eeb-a5a4-0b2266d17856"
      },
      "source": [
        "X_SMOTE[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 86715)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_jwHCm-h_lf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "e18a1e99-8469-41d9-f5a2-090860856b7d"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "estimators = [svm,rf,adab,gdbt]\n",
        "stack_model = LogisticRegression()\n",
        "stk = Stacking(estimators,stack_model,use_prob=False,n_splits=5,verbose=1)\n",
        "stk.fit(X_SMOTE, y_SMOTE)\n",
        "y_pred = stk.predict(val_tfidf)\n",
        "\n",
        "print('\\nstacking test result:\\n', classification_report(y_test, stk.predict(val_tfidf)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base Estimators are  Pipeline(memory=None,\n",
            "         steps=[('clf',\n",
            "                 LinearSVC(C=0.1, class_weight='balanced', dual=True,\n",
            "                           fit_intercept=True, intercept_scaling=1,\n",
            "                           loss='squared_hinge', max_iter=1000,\n",
            "                           multi_class='ovr', penalty='l2', random_state=None,\n",
            "                           tol=0.0001, verbose=0))],\n",
            "         verbose=False) Pipeline(memory=None,\n",
            "         steps=[('clf',\n",
            "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
            "                                        class_weight='balanced',\n",
            "                                        criterion='gini', max_depth=16,\n",
            "                                        max_features='auto',\n",
            "                                        max_leaf_nodes=None, max_samples=None,\n",
            "                                        min_impurity_decrease=0.0,\n",
            "                                        min_impurity_split=None,\n",
            "                                        min_samples_leaf=1, min_samples_split=2,\n",
            "                                        min_weight_fraction_leaf=0.0,\n",
            "                                        n_estimators=100, n_jobs=None,\n",
            "                                        oob_score=False, random_state=None,\n",
            "                                        verbose=0, warm_start=False))],\n",
            "         verbose=False)\n",
            "Stacked model is LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "False\n",
            "1  iteration\n",
            "Fitting...\n",
            "2  iteration\n",
            "Fitting...\n",
            "3  iteration\n",
            "Fitting...\n",
            "4  iteration\n",
            "Fitting...\n",
            "5  iteration\n",
            "Fitting...\n",
            "Model Fitted.\n",
            "(13326, 4)\n",
            "Predicting....\n",
            "Done.\n",
            "Predicting....\n",
            "Done.\n",
            "\n",
            "stacking test result:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.95      0.93       736\n",
            "           1       0.51      0.37      0.43       102\n",
            "\n",
            "    accuracy                           0.88       838\n",
            "   macro avg       0.71      0.66      0.68       838\n",
            "weighted avg       0.87      0.88      0.87       838\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRCeol_ihO-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "'''\n",
        "Stacking classifier\n",
        "Created on 3/4/2107\n",
        "An ensemble-learning meta-classifier for stacking\n",
        "Author: Kaushal Shetty\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "from copy import copy\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "class Stacking(object):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        estimators,\n",
        "        stacked_model,\n",
        "        use_prob=False,\n",
        "        n_splits=5,\n",
        "        verbose=0,\n",
        "        ):\n",
        "        \"\"\"\n",
        "............Stacked Generalizer Classifier\n",
        "............Trains a series of base models using K-fold cross-validation, then combines\n",
        "............the predictions of each model into a set of features that are used to train\n",
        "............a high-level classifier model. \n",
        "............Parameters\n",
        "............-----------\n",
        "............estmitators: list of classifier models\n",
        "................Each model must have a .fit and .predict_proba/.predict method a'la\n",
        "................sklearn\n",
        "............stacked_model: object\n",
        "................A classifier model used to aggregate the outputs of the trained base\n",
        "................models. Must have a .fit and .predict_proba/.predict method\n",
        "............use_prob: boolean \n",
        "................If True takes predict_proba as features else takes predict as features.\n",
        "............n_splis: int\n",
        "................The number of K-folds to use in =cross-validated model training\n",
        "............verbose: 0 or 1\n",
        "............\n",
        "........\"\"\"\n",
        "\n",
        "        super(Stacking, self).__init__()\n",
        "        if len(estimators) == 0 or stacked_model is None:\n",
        "            raise ValueError('Length of estimators must be greater than 0  and stack model connot be None type'\n",
        "                             )\n",
        "\n",
        "        if use_prob:\n",
        "            for i in range(len(estimators)):\n",
        "                if not hasattr(estimators[i], 'predict_proba'):\n",
        "                    raise AttributeError(estimators[i],\n",
        "                            ' has no attribute predict_proba. Either set use_prob to False or use a classifier that supports predict_proba'\n",
        "                            )\n",
        "        self.k = n_splits\n",
        "        self.estimators = estimators\n",
        "        self.golbal_x = None\n",
        "        self.global_y = None\n",
        "        self.stackModel = stacked_model\n",
        "        self.global_stack = None\n",
        "        self.verbose = verbose\n",
        "        self.use_prob = use_prob\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        \"\"\"\n",
        "    ........\n",
        "    ........----------\n",
        "    ........estimators : list of base models, shape = len(estimators)\n",
        "    ........stacked_model : meta ensembling model (level 2 classifier)\n",
        "    ........x :  {array-like}, shape = [n_samples, n_features selected]\n",
        "................ Training vectors, where n_samples is the number of samples and n_features is the number of features.\n",
        " \n",
        "    ........y  : array-like, shape = [n_samples]\n",
        "    ............ Target Values\n",
        "    ........Returns\n",
        "    ........-------\n",
        "    ........self : object \n",
        "    ....\"\"\"\n",
        "\n",
        "        if self.verbose > 0:\n",
        "            print ('Base Estimators are ', self.estimators[0],\n",
        "                   self.estimators[1])\n",
        "            print ('Stacked model is', self.stackModel)\n",
        "\n",
        "        self.global_x = x\n",
        "        self.global_y = y\n",
        "        kf = KFold(n_splits=self.k)\n",
        "        X_stack = None\n",
        "        cnt = 0\n",
        "        print( self.use_prob)\n",
        "        for (train_index, test_index) in kf.split(x, y):\n",
        "            cnt = cnt + 1\n",
        "\n",
        "            if self.verbose > 0:\n",
        "                print (cnt, ' iteration')\n",
        "                print ('Fitting...')\n",
        "\n",
        "            (X_train, X_test) = (x[train_index], x[test_index])\n",
        "            (y_train, y_test) = (y[train_index], y[test_index])\n",
        "            X_test_new_feats = None\n",
        "\n",
        "            for (i, estimator) in enumerate(self.estimators):\n",
        "                estimator.fit(X_train, y_train)\n",
        "\n",
        "                if self.use_prob:\n",
        "                    if i == 0:\n",
        "                        X_test_new_feats = np.column_stack((X_test,\n",
        "                                np.asarray(estimator.predict_proba(X_test),\n",
        "                                dtype=float)))\n",
        "                    else:\n",
        "                        X_test_new_feats = \\\n",
        "                            np.column_stack((X_test_new_feats,\n",
        "                                np.asarray(estimator.predict_proba(X_test),\n",
        "                                dtype=float)))\n",
        "                else:\n",
        "\n",
        "                    if i == 0:\n",
        "                        # print(X_test.shape)\n",
        "                        # print(np.asarray(estimator.predict(X_test)).shape)\n",
        "                        # X_test_new_feats = np.column_stack((X_test,\n",
        "                        #         np.asarray(estimator.predict(X_test),\n",
        "                        #         dtype=float)))\n",
        "                        X_test_new_feats = np.asarray(estimator.predict(X_test))\n",
        "                    else:\n",
        "                        \n",
        "                        X_test_new_feats = \\\n",
        "                            np.column_stack((X_test_new_feats,\n",
        "                                np.asarray(estimator.predict(X_test),\n",
        "                                dtype=float)))\n",
        "\n",
        "            if X_stack is None:\n",
        "\n",
        "                X_stack = X_test_new_feats\n",
        "            else:\n",
        "                X_stack = np.vstack((X_stack, X_test_new_feats))\n",
        "\n",
        "        if self.verbose > 0:\n",
        "            print ('Model Fitted.')\n",
        "\n",
        "        model = copy(self.stackModel)\n",
        "        print (X_stack.shape)\n",
        "        self.global_stack = X_stack\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        \"\"\" \n",
        "............Predict target values for X.\n",
        "............Parameters\n",
        "................x_test : {array-like}, shape = [n_samples, n_features]\n",
        "................ vectors, where n_samples is the number of samples and n_features is the number of features.\n",
        "............Returns\n",
        "................labels : array-like, shape = [n_samples]\n",
        "................Predicted class labels.\n",
        "\n",
        "........\"\"\"\n",
        "\n",
        "        if self.verbose > 0:\n",
        "            print ('Predicting....')\n",
        "\n",
        "        for (i, estimator) in enumerate(self.estimators):\n",
        "            estimator.fit(self.global_x, self.global_y)\n",
        "            if self.use_prob:\n",
        "                if i == 0:\n",
        "                    x_test_meta = np.column_stack((x_test,\n",
        "                            np.asarray(estimator.predict_proba(x_test),\n",
        "                            dtype=float)))\n",
        "                else:\n",
        "                    x_test_meta = np.column_stack((x_test_meta,\n",
        "                            np.asarray(estimator.predict_proba(x_test),\n",
        "                            dtype=float)))\n",
        "            else:\n",
        "\n",
        "                if i == 0:\n",
        "                    # x_test_meta = np.column_stack((x_test,\n",
        "                    #         np.asarray(estimator.predict(x_test),\n",
        "                    #         dtype=float)))\n",
        "                    x_test_meta = np.asarray(estimator.predict(x_test))\n",
        "                else:\n",
        "                    x_test_meta = np.column_stack((x_test_meta,\n",
        "                            np.asarray(estimator.predict(x_test),\n",
        "                            dtype=float)))\n",
        "\n",
        "        model = copy(self.stackModel)\n",
        "        model.fit(self.global_stack, self.global_y)\n",
        "\n",
        "        if self.verbose > 0:\n",
        "            print( 'Done.')\n",
        "\n",
        "        return np.asarray(model.predict(x_test_meta))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF20JUb4hdnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrlmKKDw_mSv",
        "colab_type": "text"
      },
      "source": [
        "# Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWErFXZhHPPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "rf_out = Pipeline([\n",
        "                ('clf', RandomForestClassifier())\n",
        "               ])\n",
        "\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "# RF best params:\n",
        "#  {'clf__max_depth': 16, 'clf__n_estimators': 100, 'tfidf__use_idf': True, 'vect__binary': True, 'vect__ngram_range': (1, 2)}\n",
        "param_grid_rf = [{\n",
        "               'clf__max_depth':[16],\n",
        "               'clf__n_estimators': [100,150,200],\n",
        "              },\n",
        "            \n",
        "             ]\n",
        "\n",
        "# cvKFold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "def goTraining(model,para,scoring=None):\n",
        "    rf_f1 = GridSearchCV(model, para,scoring=scoring,cv=10)\n",
        "    rf_f1.fit(X_SMOTE, y_SMOTE)\n",
        "    print('\\n best params:\\n', rf_f1.best_params_)\n",
        "    print('\\n test result:\\n', classification_report(y_test, rf_f1.predict(val_tfidf)))\n",
        "    print('\\n train result:\\n', classification_report(y_SMOTE, rf_f1.predict(X_SMOTE)))\n",
        "    return rf_f1\n",
        "\n",
        "rf_f1=goTraining(rf_out,param_grid_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQiEiRs1VTXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_out = Pipeline([\n",
        "               ('clf', LinearSVC(class_weight='balanced'))\n",
        "               ])\n",
        "param_grid_svm = [{\n",
        "               'clf__C':[10,1,0.1],\n",
        "               'clf__penalty':['l2']\n",
        "              }\n",
        "            ]\n",
        "svm_f1=goTraining(svm_out,param_grid_svm,'balanced_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-ZZxzAX68je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Nevertheless, lets create the final output data to upload up to kaggle for submission\n",
        "#Test our classifier on the hydrated target tweets\n",
        "def outputFile(gs_svm,df_soltion):\n",
        "    # df_soltion = df_soltion.replace(np.nan, '', regex=True)\n",
        "    predicted_targets = gs_svm.predict(test_tfidf)\n",
        "    #Set the solution to our predicted targets\n",
        "    df_soltion.tweet_label_int=predicted_targets\n",
        "    df_soltion_out=df_soltion[['tweetid','tweet_label_int']]\n",
        "    #And save it out to upload to competition!\n",
        "    df_soltion_out.to_csv(\"my_kaggle_solution.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijDdsyo1VRRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eh8Yi2d77Fl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "888cad9a-cbb9-492f-aec6-506f66eeb52c"
      },
      "source": [
        "outputFile(stk,df_soltion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting....\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q94OZxa_JOc",
        "colab_type": "text"
      },
      "source": [
        "# Embeding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4GO-2rE_I-3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eef3b956-fece-441b-808b-dd8c0244157e"
      },
      "source": [
        "# text_train_ns\n",
        "# Data after word segmentation has been processed\n",
        "len_list = [len(s) for s in train_data]\n",
        "#seq_length，Is the length of the sublist with the most words.\n",
        "seq_length = max(len_list)\n",
        "\n",
        "# add_padding\n",
        "# The function of the function is to fill in the blanks, \n",
        "# the purpose is to fill the length of all the sublists to a consistent length, \n",
        "# which uses <PAD> as the filling content.\n",
        "def add_padding(corpus, seq_length):\n",
        "    output = []\n",
        "    for sentence in corpus:\n",
        "        if len(sentence)>seq_length:\n",
        "            output.append(sentence[:seq_length])\n",
        "        else:\n",
        "            for j in range(seq_length-len(sentence)):\n",
        "                sentence.append(\"<PAD>\")\n",
        "            output.append(sentence)\n",
        "    return output\n",
        "\n",
        "text_train_pad = add_padding(train_data,seq_length )\n",
        "text_test_pad = add_padding(test_data,seq_length )\n",
        "print(len(text_test_pad))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qljb65cPX73",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "7336c56d-bc8c-4c96-f351-3de06fbd6169"
      },
      "source": [
        "text_test_pad[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['true',\n",
              " 'day',\n",
              " 'thing',\n",
              " 'stand',\n",
              " 'larg',\n",
              " 'illeg',\n",
              " 'come',\n",
              " 'countri',\n",
              " 'vaccin',\n",
              " 'kid',\n",
              " 'diseas',\n",
              " 'mutat',\n",
              " 'ignor',\n",
              " 'folk',\n",
              " 'think',\n",
              " 'vaccin',\n",
              " 'caus',\n",
              " 'autism',\n",
              " 'shouldn',\n",
              " 'forc',\n",
              " 'requir',\n",
              " 'school',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h57IL3j5G0RX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e77b78a1-b114-429b-b40d-4de07e32f487"
      },
      "source": [
        "seq_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tUZv9Om_ow2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c562d3c-e8c8-4784-b709-4dacb7f03330"
      },
      "source": [
        "import gensim.downloader as api\n",
        "word_emb_model = api.load(\"glove-twitter-100\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[================================================--] 97.3% 376.6/387.1MB downloaded"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4gi7SKF_s51",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a3f62d5b-cb2e-4bf1-857a-489aef8ad72f"
      },
      "source": [
        "def get_embeddings(corpus,word_emb_model):\n",
        "    emb_dim = word_emb_model.vector_size\n",
        "    out = []\n",
        "    for sentence in corpus:\n",
        "        out_temp = []\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                out_temp.append(word_emb_model.wv[word])\n",
        "            except:\n",
        "                out_temp.append([0]*emb_dim)\n",
        "    \n",
        "        out.append(out_temp)\n",
        "    return np.array(out)\n",
        "\n",
        "\n",
        "#embeddings\n",
        "train_emb = get_embeddings(text_train_pad,word_emb_model)\n",
        "test_emb = get_embeddings(text_test_pad,word_emb_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqfrbBxtP6l4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbb39644-7dc0-4d30-d022-ce4298c83e34"
      },
      "source": [
        "train_emb.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8372, 40, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3CXf5PCM20y",
        "colab_type": "text"
      },
      "source": [
        "# Embeding + LSTM/GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNw-xRilhpjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def labelToneHot(item):\n",
        "    label=item\n",
        "    temp=np.zeros(2)\n",
        "    temp[label]=1\n",
        "    return temp[:]\n",
        "Target=[]\n",
        "for item in df.tweet_label_int:\n",
        "    Target.append(labelToneHot(item))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXL0nzMkiDyX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "801a06c4-78cd-4a1a-c5f8-39a492f159a3"
      },
      "source": [
        "Target[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1., 0.]),\n",
              " array([1., 0.]),\n",
              " array([1., 0.]),\n",
              " array([0., 1.]),\n",
              " array([1., 0.]),\n",
              " array([1., 0.]),\n",
              " array([1., 0.]),\n",
              " array([1., 0.]),\n",
              " array([1., 0.]),\n",
              " array([1., 0.])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ank-UjPtNBDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_train, sent_val, label_train, label_val = train_test_split(train_emb, np.array(Target), test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llMBBzEEULWY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3eba8b34-a3cd-4e1a-a862-660f0ca5d902"
      },
      "source": [
        "sent_train.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6697, 40, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK1GpuiNbdgk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "12840344-37bc-477d-85dc-b071d574aba0"
      },
      "source": [
        "sent_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.093424  , -0.44723001, -0.16154   , ...,  0.2397    ,\n",
              "         0.32007   ,  0.23650999],\n",
              "       [ 0.091552  ,  0.093336  , -0.028113  , ...,  0.33173001,\n",
              "        -0.087436  , -0.55506003],\n",
              "       ...,\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouN9AOllagzF",
        "colab_type": "text"
      },
      "source": [
        "##Imbalance issue refit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q-qhOoTale8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4b2bea0e-9172-4a18-fab1-661bb1f913c6"
      },
      "source": [
        "smt = SMOTE(random_state=777, k_neighbors=1)\n",
        "X_SMOTE, y_SMOTE = smt.fit_sample(sent_train.reshape(-1,4000), label_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W07fjMytayxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_SMOTE_binary=[]\n",
        "for item in y_SMOTE:\n",
        "    y_SMOTE_binary.append(labelToneHot(item))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVEpMko0cEs2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c74fc0a2-a169-42ce-f3c1-939c7c064d1d"
      },
      "source": [
        "y_SMOTE_binary[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1., 0.]),\n",
              " array([0., 1.]),\n",
              " array([1., 0.]),\n",
              " array([1., 0.]),\n",
              " array([0., 1.]),\n",
              " array([0., 1.]),\n",
              " array([1., 0.]),\n",
              " array([1., 0.]),\n",
              " array([0., 1.]),\n",
              " array([1., 0.])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_X4phCxbXd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_SMOTE=X_SMOTE.reshape(-1,40,100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNxYMVdnbbZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "4d1ad8ad-13ad-44d1-d820-663985812d61"
      },
      "source": [
        "X_SMOTE[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.093424  , -0.44723001, -0.16154   , ...,  0.2397    ,\n",
              "         0.32007   ,  0.23650999],\n",
              "       [ 0.091552  ,  0.093336  , -0.028113  , ...,  0.33173001,\n",
              "        -0.087436  , -0.55506003],\n",
              "       ...,\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq2lOX71al_L",
        "colab_type": "text"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQhFf6w9NC0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "class trainset(Dataset):\n",
        "    def __init__(self,senetance,target):\n",
        "        \n",
        "        self.senetance = senetance\n",
        "        self.target = target\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        senetance = self.senetance[index]\n",
        "        target = self.target[index]\n",
        "        return senetance,target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.senetance)\n",
        "\n",
        "#create dataloader\n",
        "train_data = trainset(senetance=X_SMOTE,target=y_SMOTE_binary)\n",
        "trainloader = DataLoader(train_data, batch_size =2, shuffle = True, num_workers=6,pin_memory=True,drop_last=True)\n",
        "validate_data= trainset(senetance=sent_val,target=label_val)\n",
        "validateloader = DataLoader(validate_data, batch_size =2, shuffle = True, num_workers=6,pin_memory=True,drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dll4GY1oPZyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate the accuracy and recall\n",
        "def calculate_acuracy_mode_one(model_pred, labels):\n",
        "    accuracy_th = 0.5\n",
        "    pred_result = model_pred > accuracy_th\n",
        "    pred_result = pred_result.float()\n",
        "    pred_one_num = torch.sum(pred_result)\n",
        "    if pred_one_num == 0:\n",
        "        return 0.0001, 0.0001\n",
        "    target_one_num = torch.sum(labels)\n",
        "    true_predict_num = torch.sum(pred_result * labels)\n",
        "    precision = true_predict_num / pred_one_num\n",
        "    recall = true_predict_num / target_one_num\n",
        "    return precision.item(), recall.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp3llGKLP2ik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define validation function\n",
        "def evaluation_lstm(model): \n",
        "        if torch.cuda.is_available():\n",
        "            model = model.to('cuda')\n",
        "            print('using GPU')\n",
        "        else:\n",
        "            print('using CPU')\n",
        "\n",
        "        ## set model to evaluation mode\n",
        "        model.eval()\n",
        "        ## evaluation\n",
        "        ep_loss = 0.0\n",
        "        ep_acc = 0.0\n",
        "        ep_recall=0.0\n",
        "        criterion=nn.BCEWithLogitsLoss()\n",
        "        with torch.no_grad():\n",
        "            for step,(x,y) in enumerate(validateloader):\n",
        "                x=x.float()\n",
        "                y=y.float()\n",
        "                if torch.cuda.is_available():\n",
        "                    x = x.to('cuda')\n",
        "                    y = y.to('cuda')\n",
        "                # calculate output\n",
        "                p = model(x)\n",
        "                # calculate metrics\n",
        "                loss = criterion(p, y)\n",
        "\n",
        "                pred = F.sigmoid(p)\n",
        "                target = (y)\n",
        "                acc ,recall= calculate_acuracy_mode_one(pred,target)\n",
        "                # acc = accuracy(p, y)\n",
        "                ep_loss += loss\n",
        "                ep_acc += acc\n",
        "                ep_recall+=recall\n",
        "                if step % 5 == 0:\n",
        "                    print('\\rtest step: %d loss: %.2f acc: %.4f recall: %.4f F1: %.4f'\n",
        "                            % (step, ep_loss/(step+1), ep_acc/(step+1),ep_recall/(step+1), \n",
        "                             2*ep_acc/(step+1)*ep_recall/(step+1)/(ep_acc/(step+1)+ep_recall/(step+1))\n",
        "                          ),end='')\n",
        "        print('\\ntest loss: %.2f acc: %.4f recall: %.4f F1: %.4f'\n",
        "                % (ep_loss/(step+1), ep_acc/(step+1),ep_recall/(step+1),2*ep_acc/(step+1)*ep_recall/(step+1)/(ep_acc/(step+1)+ep_recall/(step+1))))\n",
        "        \n",
        "\n",
        "#define validation function\n",
        "def evaluation_gru(model): \n",
        "        if torch.cuda.is_available():\n",
        "            model = model.to('cuda')\n",
        "            print('using GPU')\n",
        "        else:\n",
        "            print('using CPU')\n",
        "\n",
        "        ## set model to evaluation mode\n",
        "        model.eval()\n",
        "        ## evaluation\n",
        "        ep_loss = 0.0\n",
        "        ep_acc = 0.0\n",
        "        ep_recall=0.0\n",
        "        criterion=nn.BCEWithLogitsLoss()\n",
        "        with torch.no_grad():\n",
        "            for step,(x,y) in enumerate(validateloader):\n",
        "                x=x.float()\n",
        "                y=y.float()\n",
        "                if torch.cuda.is_available():\n",
        "                    x = x.to('cuda')\n",
        "                    y = y.to('cuda')\n",
        "                h = model.init_hidden(16)\n",
        "                # calculate output\n",
        "                p,h = model(x,h)\n",
        "                # calculate metrics\n",
        "                loss = criterion(p, y)\n",
        "\n",
        "                pred = F.sigmoid(p)\n",
        "                target = (y)\n",
        "                acc ,recall= calculate_acuracy_mode_one(pred,target)\n",
        "                # acc = accuracy(p, y)\n",
        "                ep_loss += loss\n",
        "                ep_acc += acc\n",
        "                ep_recall+=recall\n",
        "                if step % 5 == 0:\n",
        "                    print('\\rtest step: %d loss: %.2f acc: %.4f recall: %.4f F1: %.4f'\n",
        "                            % (step, ep_loss/(step+1), ep_acc/(step+1),ep_recall/(step+1), \n",
        "                             2*ep_acc/(step+1)*ep_recall/(step+1)/(ep_acc/(step+1)+ep_recall/(step+1))\n",
        "                          ),end='')\n",
        "        print('\\ntest loss: %.2f acc: %.4f recall: %.4f F1: %.4f'\n",
        "                % (ep_loss/(step+1), ep_acc/(step+1),ep_recall/(step+1),2*ep_acc/(step+1)*ep_recall/(step+1)/(ep_acc/(step+1)+ep_recall/(step+1))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqJqWXhCQA6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "n_input = train_emb.shape[2]\n",
        "n_hidden = 100\n",
        "n_class = 2\n",
        "num_layers= 2\n",
        "total_epoch = 20\n",
        "learning_rate = 0.01\n",
        "class Lstm_Net_2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Lstm_Net_2, self).__init__()\n",
        "        self.lstm = nn.LSTM(n_input, n_hidden, num_layers,batch_first =True,bidirectional=True, dropout=0.3)\n",
        "        self.linear = nn.Linear(n_hidden*2,n_class)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        #h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n",
        "        lstm_out, (h_n,c_n) = self.lstm(sentence)\n",
        "        #concat the last hidden state from two direction\n",
        "        hidden_out =torch.cat((h_n[0,:,:],h_n[1,:,:]),1)#concatnate as colunm(left-right)\n",
        "        z = self.linear(hidden_out)\n",
        "\n",
        "        return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQpbuzaUkQ4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRUNet(nn.Module):\n",
        "    def __init__(self, input_dim=train_emb.shape[2], hidden_dim=100, output_dim=2, n_layers=2, drop_prob=0.2):\n",
        "        super(GRUNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        out, h = self.gru(x, h)\n",
        "        out = self.fc(self.relu(out[:,-1]))\n",
        "        return out, h\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        return hidden\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim=train_emb.shape[2], hidden_dim=100, output_dim=2, n_layers=2, drop_prob=0.2):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        out, h = self.lstm(x, h)\n",
        "        out = self.fc(self.relu(out[:,-1]))\n",
        "        return out, h\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26k1Nd8lNq9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#define training function\n",
        "def train_lstm(blstm):\n",
        "    epoch = 2\n",
        "    learning_rate = 0.001\n",
        "    device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\")\n",
        "    # print(device)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        blstm = blstm.to('cuda')\n",
        "        print('using GPU')\n",
        "    else:\n",
        "        print('using CPU')\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(blstm.parameters(), lr=learning_rate)\n",
        "    # optimizer = torch.optim.SGD(blstm.parameters(), learning_rate, momentum=0.9,weight_decay =1e-10)\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    batch_num = 0\n",
        "    blstm.train()\n",
        "    for ep in range(epoch):\n",
        "        ep_loss = 0.0\n",
        "        ep_acc = 0.0\n",
        "        ep_recall=0.0\n",
        "        for batch,(x,y) in enumerate(trainloader):\n",
        "            x=x.float()\n",
        "            y=y.float()\n",
        "            if torch.cuda.is_available():\n",
        "                x = x.to('cuda')\n",
        "                y = y.to('cuda')\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = blstm(x)\n",
        "            # print(type(outputs[0]))\n",
        "            # print(type(y[0]))\n",
        "            # 1/0\n",
        "            loss = criterion(outputs,y)\n",
        "            # print(outputs)\n",
        "            # print(y)\n",
        "            a=outputs\n",
        "            b=y\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            #batch_num += 1\n",
        "\n",
        "            pred = F.sigmoid(outputs)\n",
        "            target = (y)\n",
        "            # print('pred:',pred)\n",
        "            # print('target:',target)\n",
        "            acc,recall = calculate_acuracy_mode_one(pred,y)\n",
        "            # acc = accuracy(p, y)\n",
        "            # print(calculate_acuracy_mode_one(outputs,y))\n",
        "            # loss=(loss*weight).mean()\n",
        "            ep_loss += loss\n",
        "            ep_acc += acc\n",
        "            ep_recall+=recall\n",
        "            if batch % 100 == 0:\n",
        "                # print((ep_acc/(batch+1)+ep_recall/(batch+1)))\n",
        "                print('\\rtrain step: %d loss: %.2f acc: %.4f recall: %.4f F1: %.4f' \n",
        "                            % (batch, ep_loss/(batch+1), ep_acc/(batch+1),ep_recall/(batch+1),\n",
        "                               2*ep_acc/(batch+1)*ep_recall/(batch+1)/(ep_acc/(batch+1)+ep_recall/(batch+1))\n",
        "                               ), end='')\n",
        "        exp_lr_scheduler.step()\n",
        "        print('\\nepoch: %d loss: %.2f acc: %.4f recall: %.4f F1: %.4f'\n",
        "            % (ep, ep_loss/(batch+1), ep_acc/(batch+1),ep_recall/(batch+1),\n",
        "               2*ep_acc/(batch+1)*ep_recall/(batch+1)/(ep_acc/(batch+1)+ep_recall/(batch+1))\n",
        "               ))\n",
        "        print('Validation:')\n",
        "        evaluation_lstm(blstm)\n",
        "        print('\\n')\n",
        "        blstm.train()\n",
        "    return blstm\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC5_M7DxldD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#define training function\n",
        "def train_gru(gru):\n",
        "    epoch = 5\n",
        "    learning_rate = 0.001\n",
        "    batch_size=2\n",
        "    device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\")\n",
        "    # print(device)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        gru = gru.to('cuda')\n",
        "        print('using GPU')\n",
        "    else:\n",
        "        print('using CPU')\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(gru.parameters(), lr=learning_rate)\n",
        "    # optimizer = torch.optim.SGD(gru.parameters(), learning_rate, momentum=0.9,weight_decay =1e-10)\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    batch_num = 0\n",
        "    gru.train()\n",
        "    for ep in range(epoch):\n",
        "        h = gru.init_hidden(batch_size)\n",
        "        ep_loss = 0.0\n",
        "        ep_acc = 0.0\n",
        "        ep_recall=0.0\n",
        "        for batch,(x,y) in enumerate(trainloader):\n",
        "            x=x.float()\n",
        "            y=y.float()\n",
        "            if torch.cuda.is_available():\n",
        "                x = x.to('cuda')\n",
        "                y = y.to('cuda')\n",
        "            h = h.data\n",
        "            # h = tuple([e.data for e in h])\n",
        "            optimizer.zero_grad()\n",
        "            outputs,h = gru(x,h)\n",
        "            # print(type(outputs[0]))\n",
        "            # print(type(y[0]))\n",
        "            # 1/0\n",
        "            loss = criterion(outputs,y)\n",
        "            # print(outputs)\n",
        "            # print(y)\n",
        "            a=outputs\n",
        "            b=y\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            #batch_num += 1\n",
        "\n",
        "            # pred = F.sigmoid(outputs)\n",
        "            target = (y)\n",
        "            # print('pred:',pred)\n",
        "            # print('target:',target)\n",
        "            acc,recall = calculate_acuracy_mode_one(outputs,y)\n",
        "            # acc = accuracy(p, y)\n",
        "            # print(calculate_acuracy_mode_one(outputs,y))\n",
        "            # loss=(loss*weight).mean()\n",
        "            ep_loss += loss\n",
        "            ep_acc += acc\n",
        "            ep_recall+=recall\n",
        "            if batch % 100 == 0:\n",
        "                # print((ep_acc/(batch+1)+ep_recall/(batch+1)))\n",
        "                print('\\rtrain step: %d loss: %.2f acc: %.4f recall: %.4f F1: %.4f' \n",
        "                            % (batch, ep_loss/(batch+1), ep_acc/(batch+1),ep_recall/(batch+1),\n",
        "                               2*ep_acc/(batch+1)*ep_recall/(batch+1)/(ep_acc/(batch+1)+ep_recall/(batch+1))\n",
        "                               ), end='')\n",
        "        exp_lr_scheduler.step()\n",
        "        print('\\nepoch: %d loss: %.2f acc: %.4f recall: %.4f F1: %.4f'\n",
        "            % (ep, ep_loss/(batch+1), ep_acc/(batch+1),ep_recall/(batch+1),\n",
        "               2*ep_acc/(batch+1)*ep_recall/(batch+1)/(ep_acc/(batch+1)+ep_recall/(batch+1))\n",
        "               ))\n",
        "        print('Validation:')\n",
        "        evaluation_gru(gru)\n",
        "        print('\\n')\n",
        "        gru.train()\n",
        "    return gru\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7acaC8YHQJlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#training model\n",
        "# gru\n",
        "from torch.optim import lr_scheduler\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\")\n",
        "gru_2 = GRUNet().to(device)\n",
        "gru_2 = train_gru(gru_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEvBnW-gxNTF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "bf07c675-470d-4684-dc4c-6d49460c93a4"
      },
      "source": [
        "# lstm\n",
        "lstm_2 = Lstm_Net_2().to(device)\n",
        "lstm_2 = train_lstm(lstm_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train step: 2900 loss: 0.44 acc: 0.7975 recall: 0.7952 F1: 0.7963\n",
            "epoch: 0 loss: 0.44 acc: 0.7997 recall: 0.7974 F1: 0.7985\n",
            "Validation:\n",
            "using GPU\n",
            "test step: 415 loss: 0.38 acc: 0.8417 recall: 0.8413 F1: 0.8415\n",
            "test loss: 0.38 acc: 0.8425 recall: 0.8421 F1: 0.8423\n",
            "\n",
            "\n",
            "train step: 2900 loss: 0.24 acc: 0.9030 recall: 0.9025 F1: 0.9027\n",
            "epoch: 1 loss: 0.24 acc: 0.9035 recall: 0.9031 F1: 0.9033\n",
            "Validation:\n",
            "using GPU\n",
            "test step: 415 loss: 0.39 acc: 0.8495 recall: 0.8510 F1: 0.8502\n",
            "test loss: 0.39 acc: 0.8496 recall: 0.8511 F1: 0.8504\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWQ47oI3uKi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(gru_2.state_dict(), 'gru')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUNOn-xmukGm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7790879e-5fed-487a-e891-e00780415c47"
      },
      "source": [
        "gru_2 = GRUNet().to(device)\n",
        "gru_2.load_state_dict(torch.load('gru'))\n",
        "gru_2.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GRUNet(\n",
              "  (gru): GRU(100, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
              "  (fc): Linear(in_features=100, out_features=2, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJhxib2MfYwz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "024960fc-6716-449b-ff1d-6ef2aee494e3"
      },
      "source": [
        "def make_prediction_val(blstm):\n",
        "    blstm.eval()\n",
        "    with torch.no_grad():\n",
        "        test_data = torch.from_numpy(sent_val).to('cuda').float()\n",
        "        output_predicted = blstm(test_data)\n",
        "        output_predicted = F.sigmoid(output_predicted)\n",
        "        print(output_predicted)\n",
        "        accuracy_rat = 0.5\n",
        "        pre_result = output_predicted  >= accuracy_rat\n",
        "        pre_result = pre_result.float()\n",
        "    return pre_result\n",
        "val_data=make_prediction_val(lstm_2)\n",
        "pred_val=vector_to_label_lstm(val_data)\n",
        "label_val_int=vector_to_label_lstm(label_val)\n",
        "print('\\nLSTM val result:\\n', classification_report(label_val_int, pred_val))\n",
        "\n",
        "# LSTM val result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.91      0.93      0.92      1458\n",
        "#            1       0.42      0.36      0.39       217\n",
        "\n",
        "#     accuracy                           0.85      1675\n",
        "#    macro avg       0.66      0.64      0.65      1675\n",
        "# weighted avg       0.84      0.85      0.85      1675\n",
        "\n",
        "\n",
        "def make_prediction_val_gru(blstm):\n",
        "    blstm.eval()\n",
        "    with torch.no_grad():\n",
        "        test_data = torch.from_numpy(sent_val).to('cuda').float()\n",
        "        h = blstm.init_hidden(test_data.shape[0])\n",
        "        output_predicted,h = blstm(test_data,h)\n",
        "        output_predicted = F.sigmoid(output_predicted)\n",
        "        accuracy_rat = 0.5\n",
        "        pre_result = output_predicted  >= accuracy_rat\n",
        "        pre_result = pre_result.float()\n",
        "    return pre_result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.9736, 0.0278],\n",
            "        [0.8249, 0.1887],\n",
            "        [0.8874, 0.1227],\n",
            "        ...,\n",
            "        [0.6691, 0.3230],\n",
            "        [0.9351, 0.0627],\n",
            "        [0.9681, 0.0307]], device='cuda:0')\n",
            "\n",
            "LSTM val result:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.93      0.91      1458\n",
            "           1       0.40      0.31      0.35       217\n",
            "\n",
            "    accuracy                           0.85      1675\n",
            "   macro avg       0.65      0.62      0.63      1675\n",
            "weighted avg       0.84      0.85      0.84      1675\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeEAMrGxNMmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = torch.from_numpy(sent_val).to('cuda').float()\n",
        "output_predicted = lstm_2(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lz9sK1kNUJJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "a8421180-48ae-454d-d923-a4e5dcdc80f6"
      },
      "source": [
        "print(output_predicted)\n",
        "accuracy_rat = 0.5\n",
        "pre_result = output_predicted  >= accuracy_rat\n",
        "pre_result = pre_result.float()\n",
        "print(pre_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 3.3396, -3.3170],\n",
            "        [-0.8554,  0.8166],\n",
            "        [ 3.0410, -3.1327],\n",
            "        ...,\n",
            "        [ 2.3221, -2.2448],\n",
            "        [ 3.1789, -3.1770],\n",
            "        [ 2.8899, -2.8717]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        ...,\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYGK9KLUu0f7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6cd5ee2f-3cb6-48a4-9f93-f20588d63c22"
      },
      "source": [
        "#gru\n",
        "val_data=make_prediction_val_gru(gru_2)\n",
        "pred_val=vector_to_label_lstm(val_data)\n",
        "label_val_int=vector_to_label_lstm(label_val)\n",
        "print('\\nLSTM val result:\\n', classification_report(label_val_int, pred_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "LSTM val result:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.95      0.92      1458\n",
            "           1       0.40      0.24      0.30       217\n",
            "\n",
            "    accuracy                           0.86      1675\n",
            "   macro avg       0.65      0.59      0.61      1675\n",
            "weighted avg       0.83      0.86      0.84      1675\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvpVLhMPz2KV",
        "colab_type": "text"
      },
      "source": [
        "## output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHQmbAXzoBEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict the test data\n",
        "def make_prediction(blstm):\n",
        "    blstm.eval()\n",
        "    with torch.no_grad():\n",
        "        test_data = torch.from_numpy(test_emb).to('cuda').float()\n",
        "        output_predicted = blstm(test_data)\n",
        "        output_predicted = F.sigmoid(output_predicted)\n",
        "        accuracy_rat = 0.5\n",
        "        pre_result = output_predicted  >= accuracy_rat\n",
        "        pre_result = pre_result.float()\n",
        "    return pre_result\n",
        "# transfer vector to label\n",
        "def vector_to_label_lstm(vec):\n",
        "    output=[]\n",
        "    \n",
        "    for label in vec:\n",
        "        flag=0\n",
        "        if label[0]==1:\n",
        "            flag=0\n",
        "        else:\n",
        "            flag=1\n",
        "        output.append(flag)\n",
        "    return output\n",
        "#make kaggle file\n",
        "def outputFile(blstm):\n",
        "    pre_result = make_prediction(blstm)    \n",
        "    pre_lsit=vector_to_label_lstm(pre_result)\n",
        "    # create test file for kaggle\n",
        "    df_soltion.tweet_label_int=pre_lsit\n",
        "    df_soltion_out=df_soltion[['tweetid','tweet_label_int']]\n",
        "    #And save it out to upload to competition!\n",
        "    df_soltion_out.to_csv(\"my_kaggle_solution_lstm.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R4WghKoowfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre_result = make_prediction(blstm_2)    \n",
        "pre_lsit=vector_to_label_lstm(pre_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L_T02EEp3oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputFile(blstm_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKwbZls0rdHV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "78024aa1-cd62-43ff-ab25-c701d03e7e29"
      },
      "source": [
        "pre_result[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 306
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtnPERmaz5wM",
        "colab_type": "text"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSz3Cou3z7Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# _*_ coding: utf-8 _*_\n",
        "\n",
        "import torch\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(SelfAttention, self).__init__()\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\t\t\n",
        "\t\t--------\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\tself.weights = weights\n",
        "\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "\t\tself.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
        "\t\tself.dropout = 0.8\n",
        "\t\tself.bilstm = nn.LSTM(embedding_length, hidden_size, dropout=self.dropout, bidirectional=True)\n",
        "\t\t# We will use da = 350, r = 30 & penalization_coeff = 1 as per given in the self-attention original ICLR paper\n",
        "\t\tself.W_s1 = nn.Linear(2*hidden_size, 350)\n",
        "\t\tself.W_s2 = nn.Linear(350, 30)\n",
        "\t\tself.fc_layer = nn.Linear(30*2*hidden_size, 2000)\n",
        "\t\tself.label = nn.Linear(2000, output_size)\n",
        "\n",
        "\tdef attention_net(self, lstm_output):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tNow we will use self attention mechanism to produce a matrix embedding of the input sentence in which every row represents an\n",
        "\t\tencoding of the inout sentence but giving an attention to a specific part of the sentence. We will use 30 such embedding of \n",
        "\t\tthe input sentence and then finally we will concatenate all the 30 sentence embedding vectors and connect it to a fully \n",
        "\t\tconnected layer of size 2000 which will be connected to the output layer of size 2 returning logits for our two classes i.e., \n",
        "\t\tpos & neg.\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tlstm_output = A tensor containing hidden states corresponding to each time step of the LSTM network.\n",
        "\t\t---------\n",
        "\t\tReturns : Final Attention weight matrix for all the 30 different sentence embedding in which each of 30 embeddings give\n",
        "\t\t\t\t  attention to different parts of the input sentence.\n",
        "\t\tTensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\t\t\t\t  attn_weight_matrix.size() = (batch_size, 30, num_seq)\n",
        "\t\t\"\"\"\n",
        "\t\tattn_weight_matrix = self.W_s2(F.tanh(self.W_s1(lstm_output)))\n",
        "\t\tattn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
        "\t\tattn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n",
        "\n",
        "\t\treturn attn_weight_matrix\n",
        "\n",
        "\tdef forward(self, input_sentences, batch_size=None):\n",
        "\n",
        "\t\t\"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class.\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t# input = self.word_embeddings(input_sentences)\n",
        "\t\tinput = input_sentences.permute(1, 0, 2)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
        "\n",
        "\t\toutput, (h_n, c_n) = self.bilstm(input, (h_0, c_0))\n",
        "\t\toutput = output.permute(1, 0, 2)\n",
        "\t\t# output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\t# h_n.size() = (1, batch_size, hidden_size)\n",
        "\t\t# c_n.size() = (1, batch_size, hidden_size)\n",
        "\t\tattn_weight_matrix = self.attention_net(output)\n",
        "\t\t# attn_weight_matrix.size() = (batch_size, r, num_seq)\n",
        "\t\t# output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\thidden_matrix = torch.bmm(attn_weight_matrix, output)\n",
        "\t\t# hidden_matrix.size() = (batch_size, r, 2*hidden_size)\n",
        "\t\t# Let's now concatenate the hidden_matrix and connect it to the fully connected layer.\n",
        "\t\tfc_out = self.fc_layer(hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2]))\n",
        "\t\tlogits = self.label(fc_out)\n",
        "\t\t# logits.size() = (batch_size, output_size)\n",
        "\n",
        "\t\treturn logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL3kwGNBC17X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip_gradient(model, clip_value):\n",
        "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "    for p in params:\n",
        "        p.grad.data.clamp_(-clip_value, clip_value)\n",
        "def train_attn(attn):\n",
        "    epoch = 1\n",
        "    learning_rate = 0.001\n",
        "    device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\")\n",
        "    # print(device)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        attn = attn.to('cuda')\n",
        "        print('using GPU')\n",
        "    else:\n",
        "        print('using CPU')\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(attn.parameters(), lr=learning_rate)\n",
        "    # optimizer = torch.optim.SGD(attn.parameters(), learning_rate, momentum=0.9,weight_decay =1e-10)\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    batch_num = 0\n",
        "    attn.train()\n",
        "    for ep in range(epoch):\n",
        "        ep_loss = 0.0\n",
        "        ep_acc = 0.0\n",
        "        ep_recall=0.0\n",
        "        for batch,(x,y) in enumerate(trainloader):\n",
        "            x=x.float()\n",
        "            y=y.float()\n",
        "            if torch.cuda.is_available():\n",
        "                x = x.to('cuda')\n",
        "                y = y.to('cuda')\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = attn(x)\n",
        "            # print(type(outputs[0]))\n",
        "            # print(type(y[0]))\n",
        "            # 1/0\n",
        "            loss = criterion(outputs,y)\n",
        "            # print(outputs)\n",
        "            # print(y)\n",
        "            a=outputs\n",
        "            b=y\n",
        "            loss.backward()\n",
        "\n",
        "            clip_gradient(attn, 1e-1)\n",
        "\n",
        "            optimizer.step()\n",
        "            #batch_num += 1\n",
        "\n",
        "            pred = F.sigmoid(outputs)\n",
        "            target = (y)\n",
        "            # print('pred:',pred)\n",
        "            # print('target:',target)\n",
        "            acc,recall = calculate_acuracy_mode_one(pred,target)\n",
        "            # acc = accuracy(p, y)\n",
        "            # print(calculate_acuracy_mode_one(outputs,y))\n",
        "            # loss=(loss*weight).mean()\n",
        "            ep_loss += loss\n",
        "            ep_acc += acc\n",
        "            ep_recall+=recall\n",
        "            if batch % 100 == 0:\n",
        "                # print((ep_acc/(batch+1)+ep_recall/(batch+1)))\n",
        "                print('\\rtrain step: %d loss: %.2f acc: %.4f recall: %.4f F1: %.4f' \n",
        "                            % (batch, ep_loss/(batch+1), ep_acc/(batch+1),ep_recall/(batch+1),\n",
        "                               2*ep_acc/(batch+1)*ep_recall/(batch+1)/(ep_acc/(batch+1)+ep_recall/(batch+1))\n",
        "                               ), end='')\n",
        "        exp_lr_scheduler.step()\n",
        "        print('\\nepoch: %d loss: %.2f acc: %.4f recall: %.4f F1: %.4f'\n",
        "            % (ep, ep_loss/(batch+1), ep_acc/(batch+1),ep_recall/(batch+1),\n",
        "               2*ep_acc/(batch+1)*ep_recall/(batch+1)/(ep_acc/(batch+1)+ep_recall/(batch+1))\n",
        "               ))\n",
        "        print('Validation:')\n",
        "        evaluation_attn(attn)\n",
        "        print('\\n')\n",
        "        attn.train()\n",
        "    return attn\n",
        "\n",
        "def evaluation_attn(model): \n",
        "        if torch.cuda.is_available():\n",
        "            model = model.to('cuda')\n",
        "            print('using GPU')\n",
        "        else:\n",
        "            print('using CPU')\n",
        "\n",
        "        ## set model to evaluation mode\n",
        "        model.eval()\n",
        "        ## evaluation\n",
        "        ep_loss = 0.0\n",
        "        ep_acc = 0.0\n",
        "        ep_recall=0.0\n",
        "        criterion=nn.BCEWithLogitsLoss()\n",
        "        with torch.no_grad():\n",
        "            for step,(x,y) in enumerate(validateloader):\n",
        "                x=x.float()\n",
        "                y=y.float()\n",
        "                if torch.cuda.is_available():\n",
        "                    x = x.to('cuda')\n",
        "                    y = y.to('cuda')\n",
        "                # calculate output\n",
        "                p = model(x)\n",
        "                # calculate metrics\n",
        "                loss = criterion(p, y)\n",
        "\n",
        "                pred = F.sigmoid(p)\n",
        "                target = (y)\n",
        "                acc ,recall= calculate_acuracy_mode_one(pred,target)\n",
        "                # acc = accuracy(p, y)\n",
        "                ep_loss += loss\n",
        "                ep_acc += acc\n",
        "                ep_recall+=recall\n",
        "                if step % 5 == 0:\n",
        "                    print('\\rtest step: %d loss: %.2f acc: %.4f recall: %.4f F1: %.4f'\n",
        "                            % (step, ep_loss/(step+1), ep_acc/(step+1),ep_recall/(step+1), \n",
        "                             2*ep_acc/(step+1)*ep_recall/(step+1)/(ep_acc/(step+1)+ep_recall/(step+1))\n",
        "                          ),end='')\n",
        "        print('\\ntest loss: %.2f acc: %.4f recall: %.4f F1: %.4f'\n",
        "                % (ep_loss/(step+1), ep_acc/(step+1),ep_recall/(step+1),2*ep_acc/(step+1)*ep_recall/(step+1)/(ep_acc/(step+1)+ep_recall/(step+1))))\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CQYDTc2AwYT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c7bc050b-3e99-40fe-dd77-b6dc5e3195c9"
      },
      "source": [
        "batch_size = 2\n",
        "output_size = 2\n",
        "hidden_size = 256\n",
        "embedding_length = 100\n",
        "vocab_size=10000 # is not used\n",
        "word_embeddings=None # is not used\n",
        "self_atten_model=SelfAttention(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6tb0fDfE9kN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "07eda18d-b2c8-4ca3-9988-8a21dc2b49a2"
      },
      "source": [
        "self_atten_model=train_attn(self_atten_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train step: 5900 loss: 0.12 acc: 0.9605 recall: 0.9604 F1: 0.9605\n",
            "epoch: 0 loss: 0.12 acc: 0.9604 recall: 0.9603 F1: 0.9603\n",
            "Validation:\n",
            "using GPU\n",
            "test step: 835 loss: 0.67 acc: 0.8594 recall: 0.8594 F1: 0.8594\n",
            "test loss: 0.68 acc: 0.8590 recall: 0.8590 F1: 0.8590\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns6lBCEcGNAe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "dc0b00a0-f08e-4bcb-922a-5b8f1f916e97"
      },
      "source": [
        "def make_prediction_val_attn(blstm):\n",
        "    blstm.eval()\n",
        "    with torch.no_grad():\n",
        "        # test_data = torch.from_numpy(sent_val).to('cuda').float()\n",
        "        predict=[]\n",
        "        label=[]\n",
        "        for step,(x,y) in enumerate(validateloader):\n",
        "            x=x.float()\n",
        "            y=y.float()\n",
        "            if torch.cuda.is_available():\n",
        "                x = x.to('cuda')\n",
        "                y = y.to('cuda')\n",
        "            output_predicted = blstm(x)\n",
        "            output_predicted = F.sigmoid(output_predicted)\n",
        "            accuracy_rat = 0.5\n",
        "            pre_result = output_predicted  >= accuracy_rat\n",
        "            pre_result = pre_result.float()\n",
        "            # predict torch.cat([A, B], dim=0)\n",
        "            predict+=(pre_result)\n",
        "            label+=(y)\n",
        "    return predict,label\n",
        "predict,label=make_prediction_val_attn(self_atten_model)\n",
        "pred_val=vector_to_label_lstm(predict)\n",
        "label_val_int=vector_to_label_lstm(label)\n",
        "print('\\nLSTM val result:\\n', classification_report(label_val_int, pred_val))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "LSTM val result:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.91      0.90      1457\n",
            "           1       0.36      0.35      0.35       217\n",
            "\n",
            "    accuracy                           0.83      1674\n",
            "   macro avg       0.63      0.63      0.63      1674\n",
            "weighted avg       0.83      0.83      0.83      1674\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZE4y45pDkel",
        "colab_type": "text"
      },
      "source": [
        "## output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW8Ote8FDvHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_emb_data= trainset(senetance=test_emb,target=df_soltion.tweet_label_int)\n",
        "test_emb_loader = DataLoader(test_emb_data, batch_size =2, shuffle = False, num_workers=1,pin_memory=True,drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_7dHZRwTUxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict the test data\n",
        "def make_prediction_attn(blstm):\n",
        "    blstm.eval()\n",
        "    with torch.no_grad():\n",
        "        # test_data = torch.from_numpy(sent_val).to('cuda').float()\n",
        "        predict=[]\n",
        "        label=[]\n",
        "        for step,(x,y) in enumerate(test_emb_loader):\n",
        "            x=x.float()\n",
        "            # y=y.float()\n",
        "            if torch.cuda.is_available():\n",
        "                x = x.to('cuda')\n",
        "                # y = y.to('cuda')\n",
        "            output_predicted = blstm(x)\n",
        "            output_predicted = F.sigmoid(output_predicted)\n",
        "            accuracy_rat = 0.5\n",
        "            pre_result = output_predicted  >= accuracy_rat\n",
        "            pre_result = pre_result.float()\n",
        "            # predict torch.cat([A, B], dim=0)\n",
        "            predict+=(pre_result)\n",
        "           \n",
        "    return predict\n",
        "# transfer vector to label\n",
        "def vector_to_label_lstm(vec):\n",
        "    output=[]\n",
        "    \n",
        "    for label in vec:\n",
        "        flag=0\n",
        "        if label[0]==1:\n",
        "            flag=0\n",
        "        else:\n",
        "            flag=1\n",
        "        output.append(flag)\n",
        "    return output\n",
        "#make kaggle file\n",
        "def outputFile_attn(blstm):\n",
        "    pre_result = make_prediction_attn(blstm)    \n",
        "    pre_lsit=vector_to_label_lstm(pre_result)\n",
        "    # create test file for kaggle\n",
        "    df_soltion.tweet_label_int=pre_lsit\n",
        "    df_soltion_out=df_soltion[['tweetid','tweet_label_int']]\n",
        "    #And save it out to upload to competition!\n",
        "    df_soltion_out.to_csv(\"my_kaggle_solution_lstm.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXb-JGSxHNR7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0495efb7-87c0-4c2b-f418-22b41b417163"
      },
      "source": [
        "pre_result = make_prediction_attn(self_atten_model)    \n",
        "pre_lsit=vector_to_label_lstm(pre_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thxl01LbHvSd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3764d778-11ea-49ee-d94d-38f5c2136d91"
      },
      "source": [
        "len(pre_lsit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "906"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCrz8YCvE7pE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9f5970d9-11ff-4749-d251-f49ca9f108ec"
      },
      "source": [
        "outputFile_attn(self_atten_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiCX_LjWG7zX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPh8jfKaJPs3",
        "colab_type": "text"
      },
      "source": [
        "# transformer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDq6-RhaJSL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install simpletransformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCO1AVvJK-TJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import stem\n",
        "import string\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "Porter = stem.PorterStemmer()\n",
        "import re\n",
        "def remove_urls (vTEXT):\n",
        "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
        "    return(vTEXT)\n",
        "\n",
        "def remove_user(vTEXT):\n",
        "    # vTEXT = re.sub(r'(@)(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
        "    vTEXT = re.sub(r'(@)(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
        "    return(vTEXT)\n",
        "\n",
        "#token/stem/stopword_remove/ porcess\n",
        "def data_pre_transfomer(sentence_list):\n",
        "  sentences = []\n",
        "  # this is a-z and space\n",
        "  for sentence in sentence_list:\n",
        "    if sentence is np.NaN:\n",
        "          sentence=''\n",
        "    # create a remove table for invalid character\n",
        "    remove = remove_chars\n",
        "    # There are some useless char in the sentence, so here I add and delete the data\n",
        "    replace = \" \" * len(remove)\n",
        "    # make a translate table\n",
        "    remove_table = str.maketrans(remove , replace)\n",
        "    #remove url\n",
        "    senetence_removeURL=remove_urls(sentence)\n",
        "    #REMOVE USER\n",
        "    senetence_removeUrlAndUser=remove_user(senetence_removeURL)\n",
        "    # clean punctuation and digits\n",
        "    sentence_replace = senetence_removeUrlAndUser.lower().translate(remove_table)\n",
        "    #clean stopwords and tokenize\n",
        "    sentence_replace = remove_stopwords(sentence_replace)\n",
        "    #remove meaningless tokens  also remove some HTML language\n",
        "    sentence_new = []\n",
        "    # for token in sentence_replace:\n",
        "    #   # I think if a word are longer than 20, it have a great possiblity as a invalid word, and shorter than 2,it may be a stopword\n",
        "    #   # so i only get the word that the length are form 2 to 20.\n",
        "    #   if len(token) > 2 and len(token)<20:\n",
        "    #     # lemmatizer, let the word become it's original shape, had --> have\n",
        "    #     sentence_new.append(Porter.stem(token))\n",
        "    sentences.append(sentence_replace)\n",
        "  return sentences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUVVw4H4LCVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# used the data_pre method that defind above\n",
        "train_data = data_pre_transfomer(df.text)\n",
        "test_data = data_pre_transfomer(df_soltion.text)\n",
        "\n",
        "# train_data = df.text\n",
        "# test_data = df_soltion.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXXK0WQ82Omd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bbdb9b53-fc22-428f-d8a6-a8e971c6d48f"
      },
      "source": [
        "train_data[4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'vaccinating hpv costs person avert million cervical cancer deaths time deliverforgood'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX1QeVWlLojN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9026d426-1cc2-4102-9236-f6a661a9dc3b"
      },
      "source": [
        "print(df_soltion.text[13])\n",
        "print(test_data[13])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Opinion | Pinterest takes the right step toward curbing misinformation on vaccines - The Washington Post https://t.co/8rp99QK3Jk\n",
            "opinion pinterest takes right step curbing misinformation vaccines washington post\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWBHHEnWu2fv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "684b6d14-d853-436e-fbba-b2b7c182cbeb"
      },
      "source": [
        "from sklearn.utils import resample\n",
        "df_df=pd.DataFrame({'text': list(np.array(train_data)),'lables': np.array(df.tweet_label_int.values) }, columns=[ 'text','lables'])\n",
        "\n",
        "df_majority = df_df[df_df.lables==0]\n",
        "df_minority = df_df[df_df.lables==1]\n",
        "\n",
        "\n",
        "# 下采样多数类别\n",
        "df_majority_downsampled = resample(df_majority,\n",
        "                                 replace=False,    # sample without replacement\n",
        "                                 n_samples=973,     # to match minority class\n",
        "                                 random_state=123) # reproducible results\n",
        "# Combine minority class with downsampled majority class\n",
        "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
        "\n",
        "# Display new class counts\n",
        "df_downsampled.lables.value_counts()\n",
        "\n",
        "\n",
        "\n",
        "# df_minority_upsampled = resample(df_minority,\n",
        "#                                  replace=True,     # sample with replacement\n",
        "#                                  n_samples=7399,    # to match majority class\n",
        "#                                  random_state=123) # reproducible results\n",
        "# # 合并多数类别同上采样过的少数类别\n",
        "# df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "# # 显示新的类别数量\n",
        "# df_upsampled.lables.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    973\n",
              "0    973\n",
              "Name: lables, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN7ExZ8T6kZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm -rf /content/drive/My\\ Drive/DL/best_model2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6tXWKEEMGoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train_data,df.tweet_label_int.values, test_size=0.1,\n",
        "                                                    random_state=42) # so we get the same results  0.05"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eibx4Wgcttq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "923b312f-a469-42ac-990f-61facada3b8a"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAf4Dx8-QE-U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c6174cf8-f81e-466c-b458-45b4c6d32797"
      },
      "source": [
        "df_df_up=pd.DataFrame({'text': list(np.array(X_train)),'lables': np.array(y_train)}, columns=[ 'text','lables'])\n",
        "\n",
        "df_majority_up = df_df_up[df_df_up.lables==0]\n",
        "df_minority_up = df_df_up[df_df_up.lables==1]\n",
        "df_minority_upsampled = resample(df_minority_up,\n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=6663,    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "# 合并多数类别同上采样过的少数类别\n",
        "df_upsampled = pd.concat([df_majority_up, df_minority_upsampled])\n",
        "\n",
        "# 显示新的类别数量\n",
        "df_upsampled.lables.value_counts()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    6663\n",
              "0    6663\n",
              "Name: lables, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qMxgyOX0ZjP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "441e8c61-b595-4257-9e58-66615228a94b"
      },
      "source": [
        "df_df_up_eval=pd.DataFrame({'text': list(np.array(X_test)),'lables': np.array(y_test)}, columns=[ 'text','lables'])\n",
        "\n",
        "df_majority_up = df_df_up_eval[df_df_up_eval.lables==0]\n",
        "df_minority_up = df_df_up_eval[df_df_up_eval.lables==1]\n",
        "df_minority_upsampled = resample(df_minority_up,\n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=736,    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "# 合并多数类别同上采样过的少数类别\n",
        "df_upsampled_eval = pd.concat([df_majority_up, df_minority_upsampled])\n",
        "\n",
        "# 显示新的类别数量\n",
        "df_upsampled_eval.lables.value_counts()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    736\n",
              "0    736\n",
              "Name: lables, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95MVFwhOMP5v",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCdEA4LjMh0q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7dc0e9bd-45dd-411d-88be-582ccb678053"
      },
      "source": [
        "\n",
        "from sklearn.utils import shuffle\n",
        "df_upsampled = shuffle(df_upsampled)\n",
        "df_upsampled_eval = shuffle(df_upsampled_eval)\n",
        "\n",
        "\n",
        "df_upsampled_eval.lables.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    736\n",
              "0    736\n",
              "Name: lables, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG63tT_rQ4QW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# eval_df = pd.DataFrame({'text': list(np.array(X_test)),'lables': np.array(y_test) }, columns=[ 'text','lables'])\n",
        "# eval_df[-20:]\n",
        "# eval_df.lables.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IzdY4yyRr5k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d77bfbb3-e89f-49cd-fb7e-1721eeecd668"
      },
      "source": [
        "%%writefile setup.sh\n",
        "\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmRG78sV-VOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sh setup.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiDadirC1vyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import torch\n",
        "# torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-TbyE-1KQ5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reference:https://github.com/ThilinaRajapakse/simpletransformers#optional\n",
        "# the rebert model is from simpletransformers org\n",
        "from simpletransformers.classification import ClassificationModel\n",
        "from simpletransformers.classification import ClassificationArgs\n",
        "import pandas as pd\n",
        "import logging\n",
        "import sklearn\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "# Train and Evaluation data needs to be in a Pandas Dataframe of two columns. The first column is the text with type str, and the second column is the label with type int.\n",
        "# train_data = [['Example sentence belonging to class 1', 1], ['Example sentence belonging to class 0', 0]]\n",
        "# train_df = pd.DataFrame(X_train,y_train)\n",
        "\n",
        "# eval_data = [['Example eval sentence belonging to class 1', 1], ['Example eval sentence belonging to class 0', 0]]\n",
        "# eval_df = pd.DataFrame(eval_data)\n",
        "\n",
        "# Create a ClassificationModel\n",
        "model_args = ClassificationArgs()\n",
        "# model_args.do_lower_case=True\n",
        "model_args.overwrite_output_dir=True\n",
        "model_args.output_dir='output_0802'\n",
        "model_args.best_model_dir='output_0802'\n",
        "model_args.evaluate_during_training=True\n",
        "# model_args.evaluate_during_training_steps=3000\n",
        "model_args.save_eval_checkpoints=False\n",
        "model_args.warmup_steps=0\n",
        "model_args.warmup_ratio=0.006\n",
        "model_args.learning_rate=1e-6\n",
        "model_args.num_train_epochs=20\n",
        "\n",
        "# first time training \n",
        "# model_args.warmup_steps=0\n",
        "# model_args.warmup_ratio=0.006\n",
        "# model_args.learning_rate=1e-6\n",
        "# model_args.num_train_epochs=20\n",
        "\n",
        "# second time training \n",
        "# model_args.warmup_steps=0\n",
        "# model_args.warmup_ratio=0.006\n",
        "# model_args.learning_rate=1e-7\n",
        "# model_args.num_train_epochs=20\n",
        "\n",
        "model_args.train_batch_size=4\n",
        "model_args.eval_batch_size=8\n",
        "\n",
        "model_args.use_early_stopping = True\n",
        "model_args.early_stopping_delta = 0.01  \n",
        "model_args.early_stopping_metric = \"mcc\"  # disabled\n",
        "model_args.early_stopping_metric_minimize = False\n",
        "model_args.early_stopping_patience = 5\n",
        "model_args.evaluate_during_training_steps = 1000\n",
        "model_args.save_model_every_epoch=False\n",
        "model_args.save_steps=0\n",
        "\n",
        "model = ClassificationModel('roberta', '/content/drive/My Drive/DL/robert_0804', args=model_args) # You can set class weights by using the optional weight argument\n",
        "#/content/drive/My Drive/DL/robert_0731/output_new\n",
        "# Train the model\n",
        "# model.train_model(df_upsampled,eval_df=df_upsampled_eval,mcc=sklearn.metrics.matthews_corrcoef)\n",
        "# auc=sklearn.metrics.roc_auc_score\n",
        "# Evaluate the model\n",
        "#\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qezCNX-94bPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "9a2665a1-ce7d-4525-8171-3e1b68670c4d"
      },
      "source": [
        "%cp -av  /content/output_0802/ /content/drive/My\\ Drive/DL/robert_0804\n",
        "# shutil.move(\"/content/outputs/best_model\", \"/content/out/best_model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/output_0802/' -> '/content/drive/My Drive/DL/robert_0804'\n",
            "'/content/output_0802/checkpoint-3332-epoch-1' -> '/content/drive/My Drive/DL/robert_0804/checkpoint-3332-epoch-1'\n",
            "'/content/output_0802/checkpoint-3332-epoch-1/eval_results.txt' -> '/content/drive/My Drive/DL/robert_0804/checkpoint-3332-epoch-1/eval_results.txt'\n",
            "'/content/output_0802/checkpoint-6664-epoch-2' -> '/content/drive/My Drive/DL/robert_0804/checkpoint-6664-epoch-2'\n",
            "'/content/output_0802/checkpoint-6664-epoch-2/eval_results.txt' -> '/content/drive/My Drive/DL/robert_0804/checkpoint-6664-epoch-2/eval_results.txt'\n",
            "'/content/output_0802/eval_results.txt' -> '/content/drive/My Drive/DL/robert_0804/eval_results.txt'\n",
            "'/content/output_0802/training_progress_scores.csv' -> '/content/drive/My Drive/DL/robert_0804/training_progress_scores.csv'\n",
            "'/content/output_0802/config.json' -> '/content/drive/My Drive/DL/robert_0804/config.json'\n",
            "'/content/output_0802/pytorch_model.bin' -> '/content/drive/My Drive/DL/robert_0804/pytorch_model.bin'\n",
            "'/content/output_0802/tokenizer_config.json' -> '/content/drive/My Drive/DL/robert_0804/tokenizer_config.json'\n",
            "'/content/output_0802/special_tokens_map.json' -> '/content/drive/My Drive/DL/robert_0804/special_tokens_map.json'\n",
            "'/content/output_0802/vocab.json' -> '/content/drive/My Drive/DL/robert_0804/vocab.json'\n",
            "'/content/output_0802/merges.txt' -> '/content/drive/My Drive/DL/robert_0804/merges.txt'\n",
            "'/content/output_0802/training_args.bin' -> '/content/drive/My Drive/DL/robert_0804/training_args.bin'\n",
            "'/content/output_0802/optimizer.pt' -> '/content/drive/My Drive/DL/robert_0804/optimizer.pt'\n",
            "'/content/output_0802/scheduler.pt' -> '/content/drive/My Drive/DL/robert_0804/scheduler.pt'\n",
            "'/content/output_0802/model_args.json' -> '/content/drive/My Drive/DL/robert_0804/model_args.json'\n",
            "'/content/output_0802/checkpoint-9996-epoch-3' -> '/content/drive/My Drive/DL/robert_0804/checkpoint-9996-epoch-3'\n",
            "'/content/output_0802/checkpoint-9996-epoch-3/eval_results.txt' -> '/content/drive/My Drive/DL/robert_0804/checkpoint-9996-epoch-3/eval_results.txt'\n",
            "'/content/output_0802/checkpoint-13328-epoch-4' -> '/content/drive/My Drive/DL/robert_0804/checkpoint-13328-epoch-4'\n",
            "'/content/output_0802/checkpoint-13328-epoch-4/eval_results.txt' -> '/content/drive/My Drive/DL/robert_0804/checkpoint-13328-epoch-4/eval_results.txt'\n",
            "'/content/output_0802/checkpoint-16660-epoch-5' -> '/content/drive/My Drive/DL/robert_0804/checkpoint-16660-epoch-5'\n",
            "'/content/output_0802/checkpoint-16660-epoch-5/eval_results.txt' -> '/content/drive/My Drive/DL/robert_0804/checkpoint-16660-epoch-5/eval_results.txt'\n",
            "'/content/output_0802/checkpoint-19992-epoch-6' -> '/content/drive/My Drive/DL/robert_0804/checkpoint-19992-epoch-6'\n",
            "'/content/output_0802/checkpoint-19992-epoch-6/eval_results.txt' -> '/content/drive/My Drive/DL/robert_0804/checkpoint-19992-epoch-6/eval_results.txt'\n",
            "'/content/output_0802/checkpoint-23324-epoch-7' -> '/content/drive/My Drive/DL/robert_0804/checkpoint-23324-epoch-7'\n",
            "'/content/output_0802/checkpoint-23324-epoch-7/eval_results.txt' -> '/content/drive/My Drive/DL/robert_0804/checkpoint-23324-epoch-7/eval_results.txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CWe00aXKBA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.train_model(df_upsampled,eval_df=eval_df,args={'num_train_epochs': 5})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ6i3t8ZPXaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result, model_outputs, wrong_predictions = model.eval_model(eval_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB9WJwx6UQLf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319,
          "referenced_widgets": [
            "26fecc8f5a1f4ceba2bff252b6bca03b",
            "df15ef1140a24cb393b9a7cb695fe3cd",
            "88cb421e32cf454aba6cedbbac3fe3c8",
            "9de0ed61bf684d208f7fbf0f510fdc16",
            "18b02ac94c714829af3f18bb711eb5d9",
            "df8abdcc3a78454d9455ff21cc443cde",
            "c04c27dbda4941af961ce96d137f7f98",
            "15abb33144f842b7959f6cac3551f187",
            "a92648886b6d4a5eaece9f2167508803",
            "65bcc517cb824b6a8e5b204b954e9125",
            "43a4544adba441f28e41b1de24beca88",
            "335ad41501284857bdbd67360f8b7f8a",
            "dde839fdaf694c57aeeaeaf4f195894d",
            "2d505469df08431b8be7af90c29cca39",
            "4241b7c377c145ae8ac7b22b7edd33a9",
            "885fae75606a468a9eae6e40950c679a"
          ]
        },
        "outputId": "32abe730-edd7-4b9a-a45f-6bfa31e8c6a9"
      },
      "source": [
        "predictions, raw_outputs = model.predict(list(np.array(X_test)))\n",
        "predictions\n",
        "print('\\nBERT val result:\\n', classification_report(np.array(y_test), predictions))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# BERT val result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.91      0.95      0.93       362\n",
        "#            1       0.56      0.40      0.47        57\n",
        "\n",
        "#     accuracy                           0.88       419\n",
        "#    macro avg       0.74      0.68      0.70       419\n",
        "# weighted avg       0.86      0.88      0.87       419\n",
        "# 74%\n",
        "\n",
        "\n",
        "# BERT val result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.93      0.90      0.91       362\n",
        "#            1       0.46      0.54      0.50        57\n",
        "\n",
        "#     accuracy                           0.85       419\n",
        "#    macro avg       0.69      0.72      0.70       419\n",
        "# weighted avg       0.86      0.85      0.86       419\n",
        "# 64.5&\n",
        "\n",
        "\n",
        "\n",
        "# BERT val result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.91      0.93      0.92       736\n",
        "#            1       0.44      0.36      0.40       102\n",
        "\n",
        "#     accuracy                           0.87       838\n",
        "#    macro avg       0.67      0.65      0.66       838\n",
        "# weighted avg       0.86      0.87      0.86       838\n",
        "# 66.666%\n",
        "\n",
        "\n",
        "# BERT val result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.91      0.94      0.92       736\n",
        "#            1       0.43      0.34      0.38       102\n",
        "\n",
        "#     accuracy                           0.86       838\n",
        "#    macro avg       0.67      0.64      0.65       838\n",
        "# weighted avg       0.85      0.86      0.86       838\n",
        "# 69%\n",
        "\n",
        "\n",
        "# BERT val result:\n",
        "#                precision    recall  f1-score   support\n",
        "\n",
        "#            0       0.92      0.91      0.92       736\n",
        "#            1       0.42      0.45      0.44       102\n",
        "\n",
        "#     accuracy                           0.86       838\n",
        "#    macro avg       0.67      0.68      0.68       838\n",
        "# weighted avg       0.86      0.86      0.86       838\n",
        "# 80%"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26fecc8f5a1f4ceba2bff252b6bca03b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=838.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a92648886b6d4a5eaece9f2167508803",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "BERT val result:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.91      0.92       736\n",
            "           1       0.42      0.45      0.44       102\n",
            "\n",
            "    accuracy                           0.86       838\n",
            "   macro avg       0.67      0.68      0.68       838\n",
            "weighted avg       0.86      0.86      0.86       838\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt3EUMXEX8oR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132,
          "referenced_widgets": [
            "823799ee1da14ff78d57fae4d063a294",
            "717d60b5c3e7498698199ce8f3281f05",
            "6944e7b69d6845dfb94582cb046959ec",
            "06734d78518b4a0abd1abd7d63dedd66",
            "a5e41150772e42dbad8333e119d9c9c0",
            "2859040581804d949b4cb26cccabd724",
            "8034d5b05f5a4036b6aad45b62e9b5fb",
            "35d0160b9df74f40a18299e27300db2f",
            "8713dcac9b0d4faf8b54c4a6ffb112a4",
            "62b040d4c35048db8d902fdfc95776b4",
            "a3e1f92785ad431080b49e3a51acd34a",
            "50db2565dd044359b93a29fe9b086542",
            "252ff694f8c74bfebbab11b2afb10382",
            "d8cfae6b1b374c999ea30e80ee990f87",
            "9077d1b5e8eb48b88a3d90786edd7286",
            "a19be472809440cda7fcfe45d7df2030"
          ]
        },
        "outputId": "133f97d5-e5f9-4610-8a64-c71c2a24f8c0"
      },
      "source": [
        "pre_lsit, raw_outputs =model.predict(list(np.array(test_data)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "823799ee1da14ff78d57fae4d063a294",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=906.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8713dcac9b0d4faf8b54c4a6ffb112a4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=114.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K46uhRygMHW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre_lsit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErDzhrobnDyb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e7be2a7-af80-483e-e582-94ad2eb5267b"
      },
      "source": [
        "np.sum(pre_lsit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGKUERpyZR_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_soltion.tweet_label_int=pre_lsit\n",
        "df_soltion_out=df_soltion[['tweetid','tweet_label_int']]\n",
        "#And save it out to upload to competition!\n",
        "df_soltion_out.to_csv(\"/content/drive/My Drive/my_kaggle_solution_robert0804_up_leak.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJKp3b40Scql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}